<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/12/06/%E4%B8%89%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <url>/2022/12/06/%E4%B8%89%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs subunit">title: 01—三种常见激活函数<br>date: 2022<span class="hljs-string">-12</span><span class="hljs-string">-06</span> 11:24:17<br><span class="hljs-keyword">tags:</span><br>- 转载<br>categories:<br>- 深度学习基础<br></code></pre></td></tr></table></figure><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在笔试问答题或面试中<strong>偶尔</strong>有涉及到激活函数的问题，这里简单总结一下深度学习中常见的三种激活函数sigmoid、tanh和ReLU，以及它们各自的特点和用途。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="激活函数的作用是什么？"><a href="#激活函数的作用是什么？" class="headerlink" title="激活函数的作用是什么？"></a>激活函数的作用是什么？</h3><p>激活函数的主要作用是在神经网络中<strong>引入非线性因素</strong>。</p><h3 id="优秀激活函数具备的特性"><a href="#优秀激活函数具备的特性" class="headerlink" title="优秀激活函数具备的特性"></a>优秀激活函数具备的特性</h3><ul><li><strong>非线性</strong></li><li><strong>可微</strong> 基于梯度优化必备特性</li><li><strong>单调性</strong>  激活函数单调保证单层网络是凸函数，凸函数的局部最优解即为全局最优</li><li><strong>计算简单</strong> 减少计算资源</li><li><strong>非饱和性</strong> 饱和区导数近乎为0，易出现梯度消失现象</li><li><strong>值域</strong>  有限，优化稳定些；无限，训练高效，但学习率要小点</li></ul><h3 id="常见的三种激活函数"><a href="#常见的三种激活函数" class="headerlink" title="常见的三种激活函数"></a>常见的三种激活函数</h3><table><thead><tr><th align="center"></th><th align="center">sigmoid</th><th align="center">tanh</th><th align="center">ReLU</th></tr></thead><tbody><tr><td align="center">公式</td><td align="center">$f(x)=\frac{1}{1+e^{-x}}$</td><td align="center">$f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$</td><td align="center">$f(x)=max(0,x)$</td></tr><tr><td align="center">导数</td><td align="center">$f’(x)=f(x)(1-f(x))$</td><td align="center">$f’(x)=1-f^2(x)$</td><td align="center">$f’(x)=\begin{cases}1,x&gt;0\0,x\leq0\end{cases}$</td></tr><tr><td align="center">梯度消失</td><td align="center">容易造成</td><td align="center">也容易造成，但优于sigmoid</td><td align="center">可以减缓，优于前两者</td></tr><tr><td align="center">常见应用</td><td align="center">二分类任务</td><td align="center"><strong>RNN网络</strong></td><td align="center">CNN网络</td></tr><tr><td align="center">优点</td><td align="center">函数平滑，容易求导</td><td align="center">①函数平滑，容易求导<br>②输出关于零点对称</td><td align="center">①求导更快，收敛更快   <br>②有效缓解了梯度消失问题<br>③增加网络的稀疏性</td></tr><tr><td align="center">缺点</td><td align="center">①容易造成梯度消失       <br>②存在幂运算，计算量大<br>③其输出不关于零点对称</td><td align="center">①容易造成梯度消失  <br>②同样存在计算量大的问题</td><td align="center">容易造成神经元的“死亡”</td></tr><tr><td align="center">图形</td><td align="center"><img src="https://i.loli.net/2020/06/08/HIX7TKyU2MsqlbV.png"></td><td align="center"><img src="https://i.loli.net/2020/06/08/9DEFnfop1qmNM7T.png"></td><td align="center"><img src="https://i.loli.net/2020/06/08/Nc2aBh3O5pEdk4Y.png"></td></tr></tbody></table><h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><h3 id="相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？"><a href="#相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？" class="headerlink" title="相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？"></a>相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？</h3><p>对于sigmoid函数而言，其输出始终为正，这会<strong>导致在深度网络训练中模型的收敛速度变慢</strong>，因为在反向传播链式求导过程中，权重更新的效率会降低（具体推导可以参考<a href="https://www.zhihu.com/question/50396271?from=profile_question_card">这篇文章</a>）。</p><p>此外，sigmoid函数的输出均大于0，作为下层神经元的输入会导致下层输入不是0均值的，随着网络的加深可能会使得原始数据的分布发生改变。而在深度学习的网络训练中，经常需要将数据处理成零均值分布的情况，以提高收敛效率，因此tanh函数更加符合这个要求。</p><p>sigmoid函数的输出在[0,1]之间，比较适合用于二分类问题。</p><h3 id="为什么RNN中常用tanh函数作为激活函数而不是ReLU？"><a href="#为什么RNN中常用tanh函数作为激活函数而不是ReLU？" class="headerlink" title="为什么RNN中常用tanh函数作为激活函数而不是ReLU？"></a>为什么RNN中常用tanh函数作为激活函数而不是ReLU？</h3><p>详细分析可以参考<a href="https://www.zhihu.com/question/61265076/answer/186347780">这篇文章</a>。下面简单用自己的话总结一下：</p><p>RNN中将tanh函数作为激活函数本身就存在梯度消失的问题，而ReLU本就是为了克服梯度消失问题而生的，那为什么不能<strong>直接</strong>（注意：这里说的是直接替代，事实上通过<strong>截断优化</strong>ReLU仍可以在RNN中取得很好的表现）用ReLU来代替RNN中的tanh来作为激活函数呢？<strong>这是因为ReLU的导数只能为0或1，而导数为1的时候在RNN中很容易造成梯度爆炸问题</strong>。</p><p><strong>为什么会出现梯度爆炸的问题呢？</strong>因为在RNN中，每个神经元在不同的时刻都共享一个参数W（这点与CNN不同，CNN中每一层都使用独立的参数$W_i$），因此在前向和反向传播中，每个神经元的输出都会作为下一个时刻本神经元的输入，从某种意义上来讲相当于对其参数矩阵W作了连乘，如果W中有其中一个特征值大于1，则多次累乘之后的结果将非常大，自然就产生了梯度爆炸的问题。</p><p><strong>那为什么ReLU在CNN中不存在连乘的梯度爆炸问题呢？</strong>因为在CNN中，每一层都有不同的参数$W_i$，有的特征值大于1，有的小于1，在某种意义上可以理解为抵消了梯度爆炸的可能。</p><h3 id="如何解决ReLU神经元“死亡”的问题？"><a href="#如何解决ReLU神经元“死亡”的问题？" class="headerlink" title="如何解决ReLU神经元“死亡”的问题？"></a>如何解决ReLU神经元“死亡”的问题？</h3><p>①采用Leaky ReLU等激活函数    ②设置较小的学习率进行训练    ③使用momentum优化算法动态调整学习率</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://blog.csdn.net/neo_lcx/article/details/100122938">最全最详细的常见激活函数总结（sigmoid、Tanh、ReLU等）及激活函数面试常见问题总结</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2022/12/05/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2022/12/05/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<p>这是一篇测试文章</p><p><img src="%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/test.png" alt="图片引用方法二"></p>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/12/05/hello-world/"/>
    <url>/2022/12/05/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
