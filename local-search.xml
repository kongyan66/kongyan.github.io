<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>06-Pooling层的作用及如何反向传播</title>
    <link href="/2022/12/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/06_Pooling%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <url>/2022/12/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/06_Pooling%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>CNN网络在反向传播中需要逐层向前求梯度，然而<strong>pooling层没有可学习的参数</strong>，那它是如何进行反向传播的呢？</p><p>此外，CNN中为什么要加pooling层，它的作用是什么？</p><h2 id="Pooling层"><a href="#Pooling层" class="headerlink" title="Pooling层"></a>Pooling层</h2><p>CNN一般采用average pooling或max pooling来进行池化操作，而池化操作会改变feature map的大小，例如大小为64×64的feature map使用2×2的步长池化后，feature map大小为32×32。因此，这会使得在反向传播中，pooling层的梯度无法与前一层相对应。</p><p>那怎么解决这个问题呢？其实也很简单，可以理解为就是pooling操作的一个逆过程，把一个像素的梯度传递给4个像素，保证传递的loss（或梯度）总和不变。下面分别来看average pooling和max pooling的反向传播操作过程。</p><h3 id="average-pooling"><a href="#average-pooling" class="headerlink" title="average pooling"></a>average pooling</h3><p>average pooling在前向传播中，就是把一个patch中的值取平均传递给下一层的一个像素。因此，<strong>在反向传播中，就是把某个像素的值平均分成 n 份分配给上一层</strong>。（！！注意这里是分成 n 份，而不是将该元素的值复制 n 份，不然会使得loss之和变为原来的 n 倍，造成梯度爆炸。）</p><p><img src="https://i.loli.net/2020/05/15/vMwAtSe7fy5dXj4.jpg"></p><h3 id="max-pooling"><a href="#max-pooling" class="headerlink" title="max pooling"></a>max pooling</h3><p>max pooling在前向传播中，把一个patch中最大的值传递给下一层，其他值会被舍弃掉。因此，<strong>在反向传播中，就是将当前梯度直接传递给前一层的某个像素，而让同一个patch中的其他像素值为0</strong>。</p><p>所以，max pooling和average pooling不同的是，<strong>max pooling在前向传播的时候要记录池化操作时哪个像素的值是最大的</strong>，即max_id，在反向传播中才能将其对应起来。</p><p><img src="https://i.loli.net/2020/05/15/TArNvOntfCB9Goj.jpg"></p><blockquote><p>总结：    pooling层没有可学习的参数，在CNN的反向传播中，pooling层需要做的仅仅是将误差传递到上一                层，而没有计算梯度的过程。</p></blockquote><h2 id="Pooling层的作用"><a href="#Pooling层的作用" class="headerlink" title="Pooling层的作用"></a>Pooling层的作用</h2><p>两种pooling层的原理其实很容易就理解了，那它的作用又是什么呢， CNN中为什么要加pooling层？下面汇总一下几位大佬的解释：</p><p>1、<strong>增加非线性</strong></p><p>2、<strong>保留主要的特征同时减少参数(降维，效果类似PCA)和计算量，防止过拟合，提高模型泛化能力</strong></p><p>3、<strong>invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)</strong></p><h4 id="①translation-invariance（平移不变性）："><a href="#①translation-invariance（平移不变性）：" class="headerlink" title="①translation invariance（平移不变性）："></a><strong>①translation invariance（平移不变性）：</strong></h4><p>例如下面一个数字识别的例子，左边下图（大小为16×16）中的数字 1 比上图中的向右偏了一个单位，但是经过max pooling层之后，都变成了8×8的feature map。<strong>平移不变性体现在，max pooling之后，原图中的a(或b)最终都会映射到相同的位置</strong>（这句话的应该可以理解为原来feature map中的特征保持不变？比如a和b的位置不会错开，而是保持了相对位置从而保持了原来的主要特征）。</p><p>此外，图像主要的特征捕获到了，同时又将问题的规模从16×16降到了8×8（降维）。</p><p><img src="https://i.loli.net/2020/05/15/hNB69TeSsXroWq3.jpg"></p><h4 id="②rotation-invariance（旋转不变性）："><a href="#②rotation-invariance（旋转不变性）：" class="headerlink" title="②rotation invariance（旋转不变性）："></a>②rotation invariance（旋转不变性）：</h4><p>下图表示汉字“一”的识别，第一张相对于x轴有倾斜角，第二张是平行于x轴，两张图片相当于做了旋转，经过多次max pooling后具有相同的特征。</p><p><img src="https://i.loli.net/2020/05/15/XtrVphe8WIC9x6n.jpg"></p><p><strong>③scale invariance（尺度不变性）：</strong></p><p>下图表示数字“0”的识别，第一张的“0”比较大，第二张的“0”进行了较小，相当于作了缩放，同样地，经过多次max pooling后具有相同的特征。</p><p><img src="https://i.loli.net/2020/05/15/uGUTXKp6QmNlsZ2.jpg"></p><p><strong>对③scale invariance（尺度不变性）的补充理解：</strong>（来自另一位大佬，作为参考）</p><p><strong>增大了感受野</strong>！！！  怎么理解？比如上图中16×16的“0”，经过max pooling之后，可以用4×4的图表示了。</p><p>另外我们知道，CNN中利用卷积核进行卷积操作后，图像的的感受野会增大，那是不是一开始就用和图像大小一样的卷积核，获得的感受野更大，这样就更好呢？不是。因为卷积层越深模型的表征能力越强，如果直接用图像大小的卷积核就会得到1×1的feature map，一下子降维这么多，会导致很多重要信息丢失。</p><p><strong>那如果多次卷积到最后也是要降维到1×1大小，信息不是一样丢失了吗？</strong>跟直接一次降维到1×1有什么区别吗？有区别的。因为如果每次只降维一些，逐渐降维虽然信息每次都会丢失一些，但每次卷积后表征的能力就会更强一些，到最后降到1×1的时候相比于直接降到1×1还是会强一些的。</p><h4 id="pooling的缺点："><a href="#pooling的缺点：" class="headerlink" title="pooling的缺点："></a>pooling的缺点：</h4><p>pooling能够增大感受野，让卷积能看到更多的信息，但是在<strong>降维的过程中也会丢失一部分信息（只留下了它认为重要的信息）</strong>。比如对segmentation要求的精度location会有一定的影响。</p><h2 id="其他的pooling方法"><a href="#其他的pooling方法" class="headerlink" title="其他的pooling方法"></a>其他的pooling方法</h2><h3 id="overlapping-pooling（重叠池化）"><a href="#overlapping-pooling（重叠池化）" class="headerlink" title="overlapping pooling（重叠池化）"></a>overlapping pooling（重叠池化）</h3><p>重叠池化，就是相邻池化窗口之间会有重叠，即窗口大小大于步长sizeX&gt;stride。</p><h3 id="Spatial-Pyramid-Pooling（空间金字塔池化）（不是很懂）"><a href="#Spatial-Pyramid-Pooling（空间金字塔池化）（不是很懂）" class="headerlink" title="Spatial Pyramid Pooling（空间金字塔池化）（不是很懂）"></a>Spatial Pyramid Pooling（空间金字塔池化）（不是很懂）</h3><p>空间金字塔池化的思想来源于SPPNet，用大小不同的池化窗口来作用于feature map，得到1×1、2×2和4×4的池化结果，如下图所见，假设卷积层有256个filter，那么可以得到1个256维的特征、4个256维的特征、16个256维的特征。</p><p>注意：这里的1×1、2×2和4×4不是池化窗口本身的大小，而是池化后将feature map分别划分为1×1、2×2和4×4个相同大小的子区域，而要得到这样的结果，就<strong>需要根据图像的大小动态地计算出池化窗口的大小和步长</strong>。<br>$$<br>计算方法：假设conv层输出为a<em>a，要得到n</em>n的池化结果，则有：\ \sizeX=\frac{a}{n},\ \ stride=\frac{a}{n}<br>$$</p><blockquote><p>若 $\frac{a}{n}$ 刚好取得整数，自然没有问题，例如假设a=13，要得到1×1pooling结果，只需令sizeX=13，stride=13即可。</p><p>但是当 $\frac{a}{n}$ 不能取整时，例如要得到2×2pooling结果，论文中给的sizeX=7，stride=6。（应该是对窗口大小sizeX稍作调整吧，然后采用重叠池化overlapping pooling的方法进行操作）</p></blockquote><p><strong>作用：CNN中加入SPP层之后，可以让CNN处理任意大小的输入，因而模型可以变得更加灵活。</strong></p><p><img src="https://i.loli.net/2020/05/16/B81LE2Ijgsn5cF7.png"></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://blog.csdn.net/qq_21190081/article/details/72871704">深度学习笔记（3）——CNN中一些特殊环节的反向传播</a></p><p><a href="https://www.zhihu.com/question/36686900/answer/130890492">CNN网络的pooling层有什么用？</a></p><p><a href="https://blog.csdn.net/zxyhhjs2017/article/details/78607469">深度学习—之pooling层的作用与缺陷</a></p><p><a href="https://blog.csdn.net/danieljianfeng/article/details/42433475">池化方法总结</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>05-RelU在0处不可导，为何还能用？</title>
    <link href="/2022/12/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/05_ReLU%E5%87%BD%E6%95%B0%E5%9C%A80%E5%A4%84%E4%B8%8D%E5%8F%AF%E5%AF%BC%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%83%BD%E7%94%A8/"/>
    <url>/2022/12/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/05_ReLU%E5%87%BD%E6%95%B0%E5%9C%A80%E5%A4%84%E4%B8%8D%E5%8F%AF%E5%AF%BC%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%83%BD%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>ReLU函数在0处不可导，为什么在深度学习网络中还这么常用?</p><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>这是在阿里的机器学习岗一面的时候问的一个问题，最开始的问题是“<u>为什么机器学习中解决回归问题的时候一般使用平方损失（即均方误差）？</u>”。</p><p>当时我的回答是损失函数是是模型预测值与真实值之间的一种距离度量，我们可以计算出每个样本的预测值与真实值之间的距离，全部加起来就得到了所谓的损失函数。而距离的度量可以采用预测值与真实值之间差的绝对值，或者两者之差的平方，当然更高次的也行，只要你喜欢。正如问题所述，为什么我们一般使用的是两者之差的平方而不是两者只差的绝对值呢？其实这与模型的求解相关，举最简单的线性回归为例，如果采用的距离是两者之差的绝对值，那么求解的目标函数如下：<br>$$<br>(\omega^*, b) = arg min_{(\omega, b)}\sum_{i=1}^{m}\left|{f(x_i)-y_i}\right|<br>$$<br>如果采用的距离是两者之差的平方，那么求解的目标函数如下：<br>$$<br>(\omega^*, b) = arg min_{(\omega, b)}\sum_{i=1}^{m}({f(x_i)}-y_i)^2<br>$$<br>其中：$f(x_i) = \omega x_i + b$ 即预测值，$y_i$ 为真实值，$m$ 为样本总数，$\omega$ 和 $b$ 为要求解的参数</p><p>要求得使以上损失函数最小化对应的那个 $\omega$ 和 $b$ ，可将损失函数对 $\omega$ 和 $b$ 求导，并令导数为0。但是当采取的距离是两者之差的绝对值时，<strong>函数在0处不可导，且还增加的一个工作量是需要判断 $f(x_i)-y_i$ 正负号</strong>。而采用的距离是两者之差的平方时就没有这方面的问题，所以解决回归问题的时候一般使用平方损失。但理论上两者都可以使用，只是如果用两者之差的绝对值的话，那么需要判断和处理的东西多点，例如人为设定0处的导数为0等等。</p><p><strong>（以上是我当时给面试官的答案，目前还没验证对错，权当参考）</strong></p><p>回答完以上问题之后，面试官就自然而然地引出了本节要讨论的问题“<u>ReLU函数在0处不可导，为什么在深度学习网络中还这么常用?</u>“</p><h2 id="问题解答"><a href="#问题解答" class="headerlink" title="问题解答"></a>问题解答</h2><p>ReLU函数在0处确实不可导，但是在实际中却被大量使用也是事实，为什么？因为真的好用，10亿AI调参侠都在用，用了都说好。但它在0处不可导，怎么办？<strong>其实我们可以人为提供一个伪梯度，例如给它定义在0处的导数为0，其实tensorflow在实现ReLU的时候也是定义ReLU在0处的导数为0的。</strong></p><p>另外还有一个方法是使用 $ln(1+e^x)$ 来近似，这个函数是连续的，它在0点的导数是0.5。也就是相当于relu在0点的导数取为0.5，也正好是0和1的均值。</p><p>这里参考的资料有：<br><a href="https://blog.csdn.net/ningyanggege/article/details/82493023">一、relu不可微为什么可用于深度学习</a><br><a href="http://sofasofa.io/forum_main_post.php?postid=1003784">二、激活函数RELU在0点的导数是多少？</a></p><h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><p><strong>ReLU的好用体现在哪呢？下面阐述下使用ReLU函数的优势</strong></p><ol><li>ReLU函数的形式非常简洁，<code>ReLU = max(0, x)</code>，是由两段线性函数（大于0部分是线性的，小于0部分也是线性的，但是组合起来后却是非线性的）组成的非线性函数，函数形式看似简单，但ReLU的组合却可以逼近任何函数。</li><li>其实ReLU提出的<strong>最大作用</strong>是解决sigmoid函数导致的<strong>梯度消失</strong>问题的（这里对于梯度消失就不拓展，留作下个问题再具体探讨），所以ReLU的优势大部分是与它的死对头sigmoid函数对比体现出来的，对比这两个函数的图形可以看出：ReLU有单侧抑制，即Relu会使一部分神经元的输出为0，这样就造成了网络的<strong>稀疏性</strong>，并且减少了参数的相互依存关系，<strong>缓解了过拟合问题</strong>的发生。另外这也更符合生物神经元的特征。</li><li>ReLU的<strong>运算速度快</strong>，这个很明显了，max肯定比幂指数快的多。</li></ol><p><strong>当然ReLU函数也是有缺点的</strong>：可能会导致神经元死亡，权重无法更新的情况。这种神经元的死亡是不可逆转的死亡</p><p><strong>解释</strong>：训练神经网络的时候，一旦学习率没有设置好，<strong>第一次更新权重的时候，输入是负值，那么这个含有ReLU的神经节点就会死亡，再也不会被激活</strong>。因为：ReLU的导数在x&gt;0的时候是1，在x&lt;=0的时候是0。如果x&lt;=0，那么ReLU的输出是0，那么反向传播中梯度也是0，权重就不会被更新，导致神经元不再学习。<br> 也就是说，这个ReLU激活函数在训练中将不可逆转的死亡，导致了训练数据多样化的丢失。在实际训练中，如果学习率设置的太高，可能会发现网络中40%的神经元都会死掉，且在整个训练集中这些神经元都不会被激活。所以，设置一个合适的较小的学习率，会降低这种情况的发生。<strong>为了解决神经元节点死亡的情况</strong>，有人提出了Leaky ReLU、P-ReLu、R-ReLU、ELU等激活函数。</p><p>这里参考的资料有：<br><a href="https://blog.csdn.net/qq_17130909/article/details/80582226">一、激活函数及其作用以及梯度消失、爆炸、神经元节点死亡的解释</a></p><p>​                                                                                                                                                                             2022.08.30</p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>04-BN层的深入理解</title>
    <link href="/2022/12/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/04_BN%E5%B1%82%E7%9A%84%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/"/>
    <url>/2022/12/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/04_BN%E5%B1%82%E7%9A%84%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>BN在深度网络训练过程中是非常好用的trick，在笔试中也很常考，而之前只是大概知道它的作用，很多细节并不清楚，因此希望用这篇文章彻底解决揭开BN的面纱。</p><h2 id="BN层的由来与概念"><a href="#BN层的由来与概念" class="headerlink" title="BN层的由来与概念"></a>BN层的由来与概念</h2><p>讲解BN之前，我们需要了解BN是怎么被提出的。在机器学习领域，数据分布是很重要的概念。如果训练集和测试集的分布很不相同，那么在训练集上训练好的模型，在测试集上应该不奏效（比如用ImageNet训练的分类网络去在灰度医学图像上finetune再测试，效果应该不好）。对<strong>于神经网络来说，如果每一层的数据分布都不一样，后一层的网络则需要去学习适应前一层的数据分布，这相当于去做了domain的adaptation，无疑增加了训练难度，尤其是网络越来越深的情况。</strong></p><p>实际上，确实如此，不同层的输出的分布是有差异的。BN的那篇论文中指出，不同层的数据分布会往激活函数的上限或者下限偏移。论文称这种偏移为<strong>internal Covariate Shift</strong>，internal指的是网络内部。神经网络一旦训练起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变, 第一层输出变化了，势必会引起第二层输入分布的改变，模型拟合的效果就会变差，也会影响模型收敛的速度（例如我原本的参数是拟合分布A的，然后下一轮更新的时候，样本都是来自分布B的，对于这组参数来说，这些样本就会很陌生）</p><p><strong>BN就是为了解决偏移的，解决的方式也很简单，就是让每一层的分布都normalize到标准高斯分布</strong>。（BN是根据划分数据的集合去做Normalization，<strong>不同的划分方式</strong>也就出现了不同的Normalization，如GN，LN，IN）</p><h2 id="BN核心公式"><a href="#BN核心公式" class="headerlink" title="BN核心公式"></a>BN核心公式</h2><p>$$<br>Input:B={x_{1…m}}; \gamma, \beta \quad (这两个是可以训练的参数) \<br>Output : {y_i = BN_{\gamma, \beta}(x_i)} \<br>\mu_{B} \leftarrow \frac{1}{m}\sum_{i=1}^{m}{x_i} \<br>\sigma_B^2 \leftarrow \frac{1}{m}\sum_{i=1}^{m}{(x_i - \mu_B)^2} \<br>\tilde{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \varepsilon}} \quad （分母加\varepsilon是为了防止方差为0）\<br>y_i = \gamma \tilde{x}_i + \beta<br>$$</p><p>对上述公式的解释： $B$ 即一个<strong>batch</strong>中的数据，先计算 $B$ 的均值与方差，之后将 $B$ 集合的均值、方差变换为0、1即标准正态分布，最后将 $B$ 中的每个元素乘以 $\gamma$ 再加上 $\beta$ 然后输出， $\gamma$ 和 $\beta$ 是可训练的参数，这两个参数是BN层的精髓所在，为什么这么说呢？</p><p>和卷积层，激活层，全连接层一样，BN层也是属于网络中的一层。我们前面提到了，前面的层引起了数据分布的变化，这时候可能有一种思路是说：在每一层输入的时候，再加一个预处理就好。比如归一化到均值为0，方差为1，然后再输入进行学习。基本思路是这样的，然而实际上没有这么简单，如果我们只是使用简单的归一化方式：<br>$$<br>\tilde{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \varepsilon}}<br>$$<br>对某一层的输入数据做归一化，然后送入网络的下一层，这样是会影响到本层网络所学习的特征的，比如网络中学习到的数据本来大部分分布在0的右边，经过RELU激活函数以后大部分会被激活，如果直接强制归一化，那么就会有大多数的数据无法激活了，这样学习到的特征不就被破坏掉了么？论文中对上面的方法做了一些改进：<strong>变换重构</strong>，引入了可以学习的参数 $\gamma$ 和 $\beta$，这就是算法的关键之处（这两个希腊字母就是要学习的）。<br>$$<br>y_i = \gamma \tilde{x}_i + \beta<br>$$<br>每个batch的每个通道都有这样的一对参数：（看完后面应该就可以理解这句话了）<br>$$<br>\gamma = \sqrt{\sigma_B^2} \quad, \quad  \beta = \mu_B<br>$$<br>这样的时候可以恢复出原始的某一层学习到的特征的，因此我们引入这个可以学习的参数使得我们的网络可以恢复出原始网络所要学习的特征分布。</p><p><strong>我们在一些源码中，可以看到带有BN的卷积层，bias设置为False，就是因为即便卷积之后加上了Bias，在BN中也是要减去的，所以加Bias带来的非线性就被BN一定程度上抵消了。</strong></p><h2 id="BN中的均值与方差通过哪些维度计算得到"><a href="#BN中的均值与方差通过哪些维度计算得到" class="headerlink" title="BN中的均值与方差通过哪些维度计算得到"></a>BN中的均值与方差通过哪些维度计算得到</h2><p>神经网络中传递的张量数据，其维度通常记为[N, H, W, C]，其中N是batch_size，H、W是行、列，C是通道数。那么上式中BN的输入集合  $B$  就是下图中蓝色的部分。</p><img src="https://i.loli.net/2020/05/11/3ceKUWzAOrl5fv9.jpg" alt="img" style="zoom:33%;" /><p>均值的计算，就是在一个批次内，将每个通道中的数字单独加起来，再除以 $N<em>H</em>W$ 。举个栗子：该批次内有十张图片，每张图片有三个通道RGB，每张图片的高宽是 $H、W$　那么R通道的均值就是计算这十张图片R通道的像素数值总和再除以 $10<em>H</em>W$ ，其他通道类似，方差的计算也类似。</p><p>可训练参数$\gamma$ 和 $\beta$ 的维度等于张量的通道数，在上述栗子中，RGB三个通道分别需要一个$\gamma$ 和 $\beta$，所以他们的维度为３。</p><h2 id="训练与推理时BN中的均值和方差分别是多少"><a href="#训练与推理时BN中的均值和方差分别是多少" class="headerlink" title="训练与推理时BN中的均值和方差分别是多少"></a>训练与推理时BN中的均值和方差分别是多少</h2><p>正确的答案是：</p><p><strong>训练时</strong>：均值、方差分别是该批次内数据相应维度的均值与方差（<strong>实时值</strong>）</p><p><strong>推理时</strong>：均值来说直接计算所有训练时batch的 $\mu_B$ 的平均值，而方差采用训练时每个batch的 $\sigma_B^2$ 的无偏估计（<strong>历史值</strong>），公式如下：<br>$$<br>E[x] \leftarrow E_B[\mu_B] \<br>Var[x] \leftarrow \frac{m}{m-1}E_B[\sigma_B^2]<br>$$<br>但在实际实现中，如果训练几百万个Batch，那么是不是要将其均值方差全部储存，最后推理时再计算他们的均值作为推理时的均值和方差？这样显然太过笨拙，占用内存随着训练次数不断上升。为了避免该问题，后面代码实现部分使用了<strong>滑动平均</strong>，储存固定个数Batch的均值和方差，不断迭代更新推理时需要的 $E[x]$ 和 $Var[x]$  。</p><p>为了证明准确性，贴上原论文中的公式（这个图其实我都看不懂……符号好乱）：</p><img src="https://i.loli.net/2020/05/11/3hysDxtZJmdkzac.jpg　" alt="img" style="zoom:80%;" /><p>如上图第11行所示：最后测试阶段，BN采用的公式是：<br>$$<br>y = \frac{\gamma}{\sqrt{Var[x] + \varepsilon}}*x+(\beta - \frac{\gamma E[x]}{\sqrt{Var[x]+\varepsilon}})<br>$$<br>测试阶段的 $\gamma$ 和 $\beta$ 是在网络训练阶段已经学习好了的，直接加载进来计算即可。</p><h2 id="BN的好处"><a href="#BN的好处" class="headerlink" title="BN的好处"></a>BN的好处</h2><ol><li><strong>防止网络梯度消失</strong>：这个要结合sigmoid函数进行理解</li><li><strong>加速训练，也允许更大的学习率</strong>：输出分布向着激活函数的上下限偏移，带来的问题就是梯度的降低，（比如说激活函数是sigmoid），通过normalization，数据在一个合适的分布空间，经过激活函数，仍然得到不错的梯度。梯度好了自然加速训练。</li><li><strong>降低参数初始化敏感</strong>：以往模型需要设置一个不错的初始化才适合训练，加了BN就不用管这些了，现在初始化方法中随便选择一个用，训练得到的模型就能收敛。</li><li><strong>提高网络泛化能力防止过拟合</strong>：所以有了BN层，可以不再使用L2正则化和dropout。可以理解为在训练中，BN的使用使得一个mini-batch中的所有样本都被关联在了一起，因此网络不会从某一个训练样本中生成确定的结果。</li><li><strong>可以把训练数据彻底打乱</strong>（防止每批训练的时候，某一个样本都经常被挑选到，文献说这个可以提高1%的精度）。</li></ol><h2 id="代码实现BN层"><a href="#代码实现BN层" class="headerlink" title="代码实现BN层"></a>代码实现BN层</h2><p>完整代码见参考资料3</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">batch_norm</span>(<span class="hljs-params">is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):<br>    <span class="hljs-comment"># 判断当前模式是训练模式还是推理模式</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> is_training:<br>        <span class="hljs-comment"># 如果是在推理模式下，直接使用传入的移动平均所得的均值和方差</span><br>        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(X.shape) <span class="hljs-keyword">in</span> (<span class="hljs-number">2</span>, <span class="hljs-number">4</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(X.shape) == <span class="hljs-number">2</span>:<br>            <span class="hljs-comment"># 使用全连接层的情况，计算特征维上的均值和方差</span><br>            mean = X.mean(dim=<span class="hljs-number">0</span>)<br>            var = ((X - mean) ** <span class="hljs-number">2</span>).mean(dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持X的形状以便后面可以做广播运算</span><br>            <span class="hljs-comment"># torch.Tensor 高维矩阵的表示： （nSample）x C x H x W，所以对C维度外的维度求均值</span><br>            mean = X.mean(dim=<span class="hljs-number">0</span>, keepdim=<span class="hljs-literal">True</span>).mean(dim=<span class="hljs-number">2</span>, keepdim=<span class="hljs-literal">True</span>).mean(dim=<span class="hljs-number">3</span>, keepdim=<span class="hljs-literal">True</span>)<br>            var = ((X - mean) ** <span class="hljs-number">2</span>).mean(dim=<span class="hljs-number">0</span>, keepdim=<span class="hljs-literal">True</span>).mean(dim=<span class="hljs-number">2</span>, keepdim=<span class="hljs-literal">True</span>).mean(dim=<span class="hljs-number">3</span>, keepdim=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># 训练模式下用当前的均值和方差做标准化</span><br>        X_hat = (X - mean) / torch.sqrt(var + eps)<br>        <span class="hljs-comment"># 更新移动平均的均值和方差</span><br>        moving_mean = momentum * moving_mean + (<span class="hljs-number">1.0</span> - momentum) * mean<br>        moving_var = momentum * moving_var + (<span class="hljs-number">1.0</span> - momentum) * var<br>    Y = gamma * X_hat + beta  <span class="hljs-comment"># 拉伸和偏移（变换重构）</span><br>    <span class="hljs-keyword">return</span> Y, moving_mean, moving_var<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BatchNorm</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_features, num_dims</span>):　<span class="hljs-comment"># num_features就是通道数</span><br>        <span class="hljs-built_in">super</span>(BatchNorm, self).__init__()<br>        <span class="hljs-keyword">if</span> num_dims == <span class="hljs-number">2</span>:<br>            shape = (<span class="hljs-number">1</span>, num_features)<br>        <span class="hljs-keyword">else</span>:<br>            shape = (<span class="hljs-number">1</span>, num_features, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1</span><br>        self.gamma = nn.Parameter(torch.ones(shape))<br>        self.beta = nn.Parameter(torch.zeros(shape))<br>        <span class="hljs-comment"># 不参与求梯度和迭代的变量，全在内存上初始化成0</span><br>        self.moving_mean = torch.zeros(shape)<br>        self.moving_var = torch.zeros(shape)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-comment"># 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上</span><br>        <span class="hljs-keyword">if</span> self.moving_mean.device != X.device:<br>            self.moving_mean = self.moving_mean.to(X.device)<br>            self.moving_var = self.moving_var.to(X.device)<br>        <span class="hljs-comment"># 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false</span><br>        Y, self.moving_mean, self.moving_var = batch_norm(self.training,<br>            X, self.gamma, self.beta, self.moving_mean,<br>            self.moving_var, eps=<span class="hljs-number">1e-5</span>, momentum=<span class="hljs-number">0.9</span>)<br>        <span class="hljs-keyword">return</span> Y<br><br></code></pre></td></tr></table></figure><h2 id="问题延伸"><a href="#问题延伸" class="headerlink" title="问题延伸"></a>问题延伸</h2><p>当batch size越小，BN的表现效果也越不好，因为计算过程中所得到的均值和方差不能代表全局</p><p>其实深度学习中有挺多种归一化的方法，除BN外，还有LN、IN、GN和SN四种，其他四种大致了解下就行了，大同小异，这里推荐篇博客：<a href="https://blog.csdn.net/u013289254/article/details/99690730">深度学习中的五种归一化（BN、LN、IN、GN和SN）方法简介</a></p><p><img src="https://i.loli.net/2020/05/16/TgzlieXhawU1m6o.png" alt="img"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/93643523">１、六问透彻理解BN(Batch Normalization）</a><br><a href="https://www.jianshu.com/p/fcc056c1c200">２、神经网络之BN层</a><br><a href="https://blog.csdn.net/qq_36867398/article/details/103309712">３、BN层pytorch实现</a><br><a href="https://blog.csdn.net/qq_34914551/article/details/102736271?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.nonecase">４、BatchNorm的个人解读和Pytorch中BN的源码解析</a><br><a href="https://blog.csdn.net/qq_26598445/article/details/81950116">５、对于BN层的理解</a>    </p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>03-代码实现卷积操作</title>
    <link href="/2022/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/03_%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C/"/>
    <url>/2022/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/03_%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>写代码实现卷积操作</p><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>一次面试失败得来的深刻教训，自己的学习太不扎实了，理论基础薄弱，一来真格就不会。其实这个问题之前在看面经的时候就有说到过，虽然理论弄明白了，但还是心存侥幸没有动手把代码写出来……</p><h2 id="问题解答"><a href="#问题解答" class="headerlink" title="问题解答"></a>问题解答</h2><p>传统卷积运算是将卷积核以滑动窗口的方式在输入图上滑动，当前窗口内对应元素相乘然后求和得到结果，一个窗口一个结果。<strong>相乘然后求和恰好也是向量内积的计算方式</strong>，所以可以将每个窗口内的元素拉成向量，通过向量内积进行运算，多个窗口的向量放在一起就成了矩阵，每个卷积核也拉成向量，多个卷积核的向量排在一起也成了矩阵，于是，卷积运算转化成了矩阵乘法运算。下图很好地演示了矩阵乘法的运算过程：</p><p><img src="https://i.loli.net/2020/05/10/RcL8wkdDszP27pZ.png" alt="im2col"></p><p><strong>将卷积运算转化为矩阵乘法</strong>，从乘法和加法的运算次数上看，两者没什么差别，但是转化成矩阵后，运算时需要的数据被存在连续的内存上，这样访问速度大大提升（cache），同时，矩阵乘法有很多库提供了高效的实现方法，像BLAS、MKL等，转化成矩阵运算后可以通过这些库进行加速。</p><p>缺点呢？这是一种<strong>空间换时间的方法</strong>，<strong>消耗了更多的内存</strong>——转化的过程中数据被冗余存储。</p><p>还有两张形象化的图片帮助理解：</p><p><img src="https://i.loli.net/2020/05/10/OE42zDJtGp5M9HY.png" alt="这里写图片描述"></p><p><img src="https://i.loli.net/2020/05/10/XWVOe163Hf5ijpY.png" alt="这里写图片描述"></p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>太久没写python代码，面试的时候居然想用c++来实现，其实肯定能实现，但是比起使用python复杂太多了，所以这里使用python中的numpy来实现。</p><p>一、滑动窗口版本实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python3    #加上这一句之后，在终端命令行模式下就可以直接输入这个文件的名字后运行文件中的代码</span><br><span class="hljs-comment"># -*- coding = utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 为了简化运算，默认batch_size = 1</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">my_conv</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_data, weight_data, stride, padding = <span class="hljs-string">&#x27;SAME&#x27;</span></span>):<br>        self.<span class="hljs-built_in">input</span> = np.asarray(input_data, np.float32)<br>        self.weights = np.asarray(weight_data, np.float32)<br>        self.stride = stride<br>        self.padding = padding<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">my_conv2d</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        self.input: c * h * w  # 输入的数据格式</span><br><span class="hljs-string">        self.weights: c * h * w</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        [c, h, w] = self.<span class="hljs-built_in">input</span>.shape<br>        [kc, k, _] = self.weights.shape  <span class="hljs-comment"># 这里默认卷积核的长宽相等</span><br>        <span class="hljs-keyword">assert</span> c == kc  <span class="hljs-comment"># 如果输入的channel与卷积核的channel不一致即报错</span><br>        output = []<br>        <span class="hljs-comment"># 分通道卷积，最后再加起来</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(c):  <br>            f_map = self.<span class="hljs-built_in">input</span>[i]<br>            kernel = self.weights[i]<br>            rs = self.compute_conv(f_map, kernel)<br>            <span class="hljs-keyword">if</span> output == []:<br>                output = rs<br>            <span class="hljs-keyword">else</span>:<br>                output += rs<br>        <span class="hljs-keyword">return</span> output<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_conv</span>(<span class="hljs-params">self, fm, kernel</span>):<br>        [h, w] = fm.shape<br>        [k, _] = kernel.shape<br><br>        <span class="hljs-keyword">if</span> self.padding == <span class="hljs-string">&#x27;SAME&#x27;</span>:<br>            pad_h = (self.stride * (h - <span class="hljs-number">1</span>) + k - h) // <span class="hljs-number">2</span><br>            pad_w = (self.stride * (w - <span class="hljs-number">1</span>) + k - w) // <span class="hljs-number">2</span><br>            rs_h = h<br>            rs_w = w<br>        <span class="hljs-keyword">elif</span> self.padding == <span class="hljs-string">&#x27;VALID&#x27;</span>:<br>            pad_h = <span class="hljs-number">0</span><br>            pad_w = <span class="hljs-number">0</span><br>            rs_h = (h - k) // self.stride + <span class="hljs-number">1</span><br>            rs_w = (w - k) // self.stride + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> self.padding == <span class="hljs-string">&#x27;FULL&#x27;</span>:<br>            pad_h = k - <span class="hljs-number">1</span><br>            pad_w = k - <span class="hljs-number">1</span><br>            rs_h = (h + <span class="hljs-number">2</span> * pad_h - k) // self.stride + <span class="hljs-number">1</span><br>            rs_w = (w + <span class="hljs-number">2</span> * pad_w - k) // self.stride + <span class="hljs-number">1</span><br>        padding_fm = np.zeros([h + <span class="hljs-number">2</span> * pad_h, w + <span class="hljs-number">2</span> * pad_w], np.float32)<br>        padding_fm[pad_h:pad_h+h, pad_w:pad_w+w] = fm  <span class="hljs-comment"># 完成对fm的zeros padding</span><br>        rs = np.zeros([rs_h, rs_w], np.float32)<br><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(rs_h):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(rs_w):<br>                roi = padding_fm[i*self.stride:(i*self.stride + k), j*self.stride:(j*self.stride + k)]<br>                rs[i, j] = np.<span class="hljs-built_in">sum</span>(roi * kernel) <span class="hljs-comment"># np.asarray格式下的 * 是对应元素相乘</span><br>        <span class="hljs-keyword">return</span> rs<br><br><span class="hljs-keyword">if</span> __name__==<span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    input_data = [<br>        [<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>],<br>            [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>            [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>],<br>        ],<br>        [<br>            [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>],<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br><br>        ],<br>    ]<br>    weight_data = [<br>        [<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>            [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>            [<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>        ],<br>        [<br>            [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>        ]<br>    ]<br>    conv = my_conv(input_data, weight_data, <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;SAME&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(conv.my_conv2d())<br></code></pre></td></tr></table></figure><p>二、矩阵乘法版本实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python3    #加上这一句之后，在终端命令行模式下就可以直接输入这个文件的名字后运行文件中的代码</span><br><span class="hljs-comment"># _*_ coding = utf-8 _*_</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 为了简化运算，默认batch_size = 1</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">my_conv</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_data, weight_data, stride, padding = <span class="hljs-string">&#x27;SAME&#x27;</span></span>):<br>        self.<span class="hljs-built_in">input</span> = np.asarray(input_data, np.float32)<br>        self.weights = np.asarray(weight_data, np.float32)<br>        self.stride = stride<br>        self.padding = padding<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">my_conv2d</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        self.input: c * h * w  # 输入的数据格式</span><br><span class="hljs-string">        self.weights: c * h * w</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        [c, h, w] = self.<span class="hljs-built_in">input</span>.shape<br>        [kc, k, _] = self.weights.shape  <span class="hljs-comment"># 这里默认卷积核的长宽相等</span><br>        <span class="hljs-keyword">assert</span> c == kc  <span class="hljs-comment"># 如果输入的channel与卷积核的channel不一致即报错</span><br>        <span class="hljs-comment"># rs_h与rs_w为最后输出的feature map的高与宽</span><br>        <span class="hljs-keyword">if</span> self.padding == <span class="hljs-string">&#x27;SAME&#x27;</span>:<br>            pad_h = (self.stride * (h - <span class="hljs-number">1</span>) + k - h) // <span class="hljs-number">2</span><br>            pad_w = (self.stride * (w - <span class="hljs-number">1</span>) + k - w) // <span class="hljs-number">2</span><br>            rs_h = h<br>            rs_w = w<br>        <span class="hljs-keyword">elif</span> self.padding == <span class="hljs-string">&#x27;VALID&#x27;</span>:<br>            pad_h = <span class="hljs-number">0</span><br>            pad_w = <span class="hljs-number">0</span><br>            rs_h = (h - k) // self.stride + <span class="hljs-number">1</span><br>            rs_w = (w - k) // self.stride + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> self.padding == <span class="hljs-string">&#x27;FULL&#x27;</span>:<br>            pad_h = k - <span class="hljs-number">1</span><br>            pad_w = k - <span class="hljs-number">1</span><br>            rs_h = (h + <span class="hljs-number">2</span> * pad_h - k) // self.stride + <span class="hljs-number">1</span><br>            rs_w = (w + <span class="hljs-number">2</span> * pad_w - k) // self.stride + <span class="hljs-number">1</span><br>        <span class="hljs-comment"># 对输入进行zeros padding，注意padding后依然是三维的</span><br>        pad_fm = np.zeros([c, h+<span class="hljs-number">2</span>*pad_h, w+<span class="hljs-number">2</span>*pad_w], np.float32)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(c):<br>            pad_fm[i, pad_h:pad_h+h, pad_w:pad_w+w] = self.<span class="hljs-built_in">input</span>[i]<br>        <span class="hljs-comment"># 将输入和卷积核转化为矩阵相乘的规格</span><br>        mat_fm = np.zeros([rs_h*rs_w, kc*k*k], np.float32)<br>        mat_kernel = self.weights<br>        mat_kernel.shape = (kc*k*k, <span class="hljs-number">1</span>) <span class="hljs-comment"># 转化为列向量</span><br>        row = <span class="hljs-number">0</span>   <br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(rs_h):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(rs_w):<br>                roi = pad_fm[:, i*self.stride:(i*self.stride+k), j*self.stride:(j*self.stride+k)]<br>                mat_fm[row] = roi.flatten()  <span class="hljs-comment"># 将roi扁平化，即变为行向量</span><br>                row += <span class="hljs-number">1</span><br>        <span class="hljs-comment"># 卷积的矩阵乘法实现</span><br>        rs = np.dot(mat_fm, mat_kernel).reshape(rs_h, rs_w) <br>        <span class="hljs-keyword">return</span> rs<br><br><span class="hljs-keyword">if</span> __name__==<span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    input_data = [<br>        [<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>],<br>            [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>            [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>],<br>        ],<br>        [<br>            [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>],<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br><br>        ],<br>    ]<br>    weight_data = [<br>        [<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>            [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>            [<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>        ],<br>        [<br>            [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>        ]<br>    ]<br>    conv = my_conv(input_data, weight_data, <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;SAME&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(conv.my_conv2d())<br></code></pre></td></tr></table></figure><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://www.cnblogs.com/shine-lee/p/10775831.html">１、im2col：将卷积运算转为矩阵相乘</a><br><a href="https://blog.csdn.net/Biyoner/article/details/88916247">２、面试基础–深度学习　卷积及其代码实现</a></p><p>​                                                                                                                                                            </p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>02—过拟合与欠拟合表现与解决方法</title>
    <link href="/2022/12/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/02_%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88%E7%9A%84%E8%A1%A8%E7%8E%B0%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
    <url>/2022/12/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/02_%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88%E7%9A%84%E8%A1%A8%E7%8E%B0%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>过拟合和欠拟合的表现和解决方法。</p><p>其实除了欠拟合和过拟合，还有一种是<strong>适度拟合</strong>，适度拟合就是我们模型训练想要达到的状态，不过适度拟合这个词平时真的好少见，在做酷狗音乐的笔试题时还懵逼了一会，居然还真的有这样的说法。</p><p>这应该是基础中的基础了，笔试题都做烂了。那就当做今天周末，继续放个假吧……</p><p><img src="https://i.loli.net/2020/05/16/m1MXWUG6RZEfB2H.jpg"></p><h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><h3 id="过拟合的表现"><a href="#过拟合的表现" class="headerlink" title="过拟合的表现"></a>过拟合的表现</h3><p>模型在训练集上的表现非常好，但是在测试集、验证集以及新数据上的表现很差，损失曲线呈现一种<strong>高方差</strong>状态。(高方差指的是训练集误差较低，而测试集误差比训练集大较多)</p><img src="https://i.loli.net/2020/05/16/dNxegZrbA6GUwPp.png" alt="img" style="zoom:150%;" /><h3 id="过拟合的原因"><a href="#过拟合的原因" class="headerlink" title="过拟合的原因"></a>过拟合的原因</h3><p>从两个角度去分析：</p><ol><li><strong>模型的复杂度</strong>：模型过于复杂，把噪声数据的特征也学习到模型中，导致模型泛化性能下降</li><li><strong>数据集规模大小</strong>：数据集规模相对模型复杂度来说太小，使得模型过度挖掘数据集中的特征，把一些不具有代表性的特征也学习到了模型中。例如训练集中有一个叶子图片，该叶子的边缘是锯齿状，模型学习了该图片后认为叶子都应该有锯齿状边缘，因此当新数据中的叶子边缘不是锯齿状时，都判断为不是叶子。</li></ol><h3 id="过拟合的解决方法"><a href="#过拟合的解决方法" class="headerlink" title="过拟合的解决方法"></a>过拟合的解决方法</h3><ol><li><p><strong>获得更多的训练数据</strong>：使用更多的训练数据是解决过拟合问题最有效的手段，因为更多的样本能够让模型学习到更多更有效的特征，减少噪声的影响。</p><p>当然直接增加实验数据在很多场景下都是没那么容易的，因此可以通过<strong>数据扩充技术</strong>，例如对图像进行平移、旋转和缩放等等。</p><p>除了根据原有数据进行扩充外，还有一种思路是使用非常火热的**生成式对抗网络 GAN **来合成大量的新训练数据。</p><p>还有一种方法是使用<strong>迁移学习技术</strong>，使用已经在更大规模的源域数据集上训练好的模型参数来初始化我们的模型，模型往往可以更快地收敛。但是也有一个问题是，源域数据集中的场景跟我们目标域数据集的场景差异过大时，可能效果会不太好，需要多做实验来判断。</p></li><li><p><strong>降低模型复杂度</strong>：在深度学习中我们可以减少网络的层数，改用参数量更少的模型；在机器学习的决策树模型中可以降低树的高度、进行剪枝等。</p></li><li><p><strong>正则化方法</strong>如 L2 将权值大小加入到损失函数中，根据奥卡姆剃刀原理，拟合效果差不多情况下，模型复杂度越低越好。至于为什么正则化可以减轻过拟合这个问题可以看看<a href="https://blog.csdn.net/qq_37344125/article/details/104326946">这个博客</a>，挺好懂的.。</p><p><strong>添加BN层</strong>（这个我们专门在BN专题中讨论过了，BN层可以一定程度上提高模型泛化性能）</p><p>使用<strong>dropout技术</strong>（dropout在训练时会随机隐藏一些神经元，导致训练过程中不会每次都更新(<strong>预测时不会发生dropout</strong>)，最终的结果是每个神经元的权重w都不会更新的太大，起到了类似L2正则化的作用来降低过拟合风险。）</p></li><li><p><strong>Early Stopping</strong>：Early stopping便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。</p><p> Early stopping方法的具体做法是：在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。这种做法很符合直观感受，因为accurary都不再提高了，在继续训练也是无益的，只会提高训练的时间。那么该做法的一个重点便是怎样才认为validation accurary不再提高了呢？并不是说validation accuracy一降下来便认为不再提高了，因为可能经过这个Epoch后，accuracy降低了，但是随后的Epoch又让accuracy又上去了，所以不能根据一两次的连续降低就判断不再提高。一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。</p></li><li><p><strong>集成学习方法</strong>：集成学习是把多个模型集成在一起，来降低单一模型的过拟合风险，例如Bagging方法。</p><p>如DNN可以用Bagging的思路来正则化。首先我们要对原始的m个训练样本进行有放回随机采样，构建N组m个样本的数据集，然后分别用这N组数据集去训练我们的DNN。即采用我们的前向传播算法和反向传播算法得到N个DNN模型的W,b参数组合，最后对N个DNN模型的输出用加权平均法或者投票法决定最终输出。不过用集成学习Bagging的方法有一个问题，就是我们的DNN模型本来就比较复杂，参数很多。现在又变成了N个DNN模型，这样参数又增加了N倍，从而导致训练这样的网络要花更加多的时间和空间。因此一般N的个数不能太多，比如5-10个就可以了。</p></li><li><p><strong>交叉检验</strong>，如S折交叉验证，通过交叉检验得到较优的模型参数，其实这个跟上面的Bagging方法比较类似，只不过S折交叉验证是随机将已给数据切分成S个互不相交的大小相同的自己，然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出S次评测中平均测试误差最小的模型。</p></li></ol><h2 id="欠拟合"><a href="#欠拟合" class="headerlink" title="欠拟合"></a>欠拟合</h2><h3 id="欠拟合的表现"><a href="#欠拟合的表现" class="headerlink" title="欠拟合的表现"></a>欠拟合的表现</h3><p>模型无论是在训练集还是在测试集上的表现都很差，损失曲线呈现一种<strong>高偏差</strong>状态。（高偏差指的是训练集和验证集的误差都较高，但相差很少）</p><img src="https://i.loli.net/2020/05/16/N2v1dDnKqfk7iQs.png" alt="img" style="zoom:150%;" /><h3 id="欠拟合的原因"><a href="#欠拟合的原因" class="headerlink" title="欠拟合的原因"></a>欠拟合的原因</h3><p>同样可以从两个角度去分析：</p><ol><li><strong>模型过于简单</strong>：简单模型的学习能力比较差</li><li><strong>提取的特征不好</strong>：当特征不足或者现有特征与样本标签的相关性不强时，模型容易出现欠拟合</li></ol><h3 id="欠拟合的解决方法"><a href="#欠拟合的解决方法" class="headerlink" title="欠拟合的解决方法"></a>欠拟合的解决方法</h3><ol><li><strong>增加模型复杂度</strong>：如线性模型增加高次项改为非线性模型、在神经网络模型中增加网络层数或者神经元个数、深度学习中改为使用参数量更多更先进的模型等等。</li><li><strong>增加新特征</strong>：可以考虑特征组合等特征工程工作（这主要是针对机器学习而言，特征工程还真不太了解……）</li><li>如果损失函数中加了正则项，可以考虑<strong>减小正则项的系数</strong> $\lambda$</li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.jianshu.com/p/f2489ccc14b4">过拟合与欠拟合及方差偏差</a>   (这个博客总结地很好，可以看看)<br><u>《百面机器学习》</u><br><a href="https://blog.csdn.net/u012197749/article/details/79766317">机器学习+过拟合和欠拟合+方差和偏差</a><br><a href="https://blog.csdn.net/GL3_24/article/details/90294490">如何判断欠拟合、适度拟合、过拟合</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>01—三种常见激活函数</title>
    <link href="/2022/12/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/01_%E4%B8%89%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <url>/2022/12/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/01_%E4%B8%89%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在笔试问答题或面试中<strong>偶尔</strong>有涉及到激活函数的问题，这里简单总结一下深度学习中常见的三种激活函数sigmoid、tanh和ReLU，以及它们各自的特点和用途。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="激活函数的作用是什么？"><a href="#激活函数的作用是什么？" class="headerlink" title="激活函数的作用是什么？"></a>激活函数的作用是什么？</h3><p>激活函数的主要作用是在神经网络中<strong>引入非线性因素</strong>。</p><h3 id="优秀激活函数具备的特性"><a href="#优秀激活函数具备的特性" class="headerlink" title="优秀激活函数具备的特性"></a>优秀激活函数具备的特性</h3><ul><li><strong>非线性</strong></li><li><strong>可微</strong> 基于梯度优化必备特性</li><li><strong>单调性</strong>  激活函数单调保证单层网络是凸函数，凸函数的局部最优解即为全局最优</li><li><strong>计算简单</strong> 减少计算资源</li><li><strong>非饱和性</strong> 饱和区导数近乎为0，易出现梯度消失现象</li><li><strong>值域</strong>  有限，优化稳定些；无限，训练高效，但学习率要小点</li></ul><h3 id="常见的三种激活函数"><a href="#常见的三种激活函数" class="headerlink" title="常见的三种激活函数"></a>常见的三种激活函数</h3><table><thead><tr><th align="center"></th><th align="center">sigmoid</th><th align="center">tanh</th><th align="center">ReLU</th></tr></thead><tbody><tr><td align="center">公式</td><td align="center">$f(x)=\frac{1}{1+e^{-x}}$</td><td align="center">$f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$</td><td align="center">$f(x)=max(0,x)$</td></tr><tr><td align="center">导数</td><td align="center">$f’(x)=f(x)(1-f(x))$</td><td align="center">$f’(x)=1-f^2(x)$</td><td align="center">$f’(x)=\begin{cases}1,x&gt;0\0,x\leq0\end{cases}$</td></tr><tr><td align="center">梯度消失</td><td align="center">容易造成</td><td align="center">也容易造成，但优于sigmoid</td><td align="center">可以减缓，优于前两者</td></tr><tr><td align="center">常见应用</td><td align="center">二分类任务</td><td align="center"><strong>RNN网络</strong></td><td align="center">CNN网络</td></tr><tr><td align="center">优点</td><td align="center">函数平滑，容易求导</td><td align="center">①函数平滑，容易求导<br>②输出关于零点对称</td><td align="center">①求导更快，收敛更快   <br>②有效缓解了梯度消失问题<br>③增加网络的稀疏性</td></tr><tr><td align="center">缺点</td><td align="center">①容易造成梯度消失       <br>②存在幂运算，计算量大<br>③其输出不关于零点对称</td><td align="center">①容易造成梯度消失  <br>②同样存在计算量大的问题</td><td align="center">容易造成神经元的“死亡”</td></tr><tr><td align="center">图形</td><td align="center"><img src="https://i.loli.net/2020/06/08/HIX7TKyU2MsqlbV.png"></td><td align="center"><img src="https://i.loli.net/2020/06/08/9DEFnfop1qmNM7T.png"></td><td align="center"><img src="https://i.loli.net/2020/06/08/Nc2aBh3O5pEdk4Y.png"></td></tr></tbody></table><h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><h3 id="相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？"><a href="#相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？" class="headerlink" title="相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？"></a>相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？</h3><p>对于sigmoid函数而言，其输出始终为正，这会<strong>导致在深度网络训练中模型的收敛速度变慢</strong>，因为在反向传播链式求导过程中，权重更新的效率会降低（具体推导可以参考<a href="https://www.zhihu.com/question/50396271?from=profile_question_card">这篇文章</a>）。</p><p>此外，sigmoid函数的输出均大于0，作为下层神经元的输入会导致下层输入不是0均值的，随着网络的加深可能会使得原始数据的分布发生改变。而在深度学习的网络训练中，经常需要将数据处理成零均值分布的情况，以提高收敛效率，因此tanh函数更加符合这个要求。</p><p>sigmoid函数的输出在[0,1]之间，比较适合用于二分类问题。</p><h3 id="为什么RNN中常用tanh函数作为激活函数而不是ReLU？"><a href="#为什么RNN中常用tanh函数作为激活函数而不是ReLU？" class="headerlink" title="为什么RNN中常用tanh函数作为激活函数而不是ReLU？"></a>为什么RNN中常用tanh函数作为激活函数而不是ReLU？</h3><p>详细分析可以参考<a href="https://www.zhihu.com/question/61265076/answer/186347780">这篇文章</a>。下面简单用自己的话总结一下：</p><p>RNN中将tanh函数作为激活函数本身就存在梯度消失的问题，而ReLU本就是为了克服梯度消失问题而生的，那为什么不能<strong>直接</strong>（注意：这里说的是直接替代，事实上通过<strong>截断优化</strong>ReLU仍可以在RNN中取得很好的表现）用ReLU来代替RNN中的tanh来作为激活函数呢？<strong>这是因为ReLU的导数只能为0或1，而导数为1的时候在RNN中很容易造成梯度爆炸问题</strong>。</p><p><strong>为什么会出现梯度爆炸的问题呢？</strong>因为在RNN中，每个神经元在不同的时刻都共享一个参数W（这点与CNN不同，CNN中每一层都使用独立的参数$W_i$），因此在前向和反向传播中，每个神经元的输出都会作为下一个时刻本神经元的输入，从某种意义上来讲相当于对其参数矩阵W作了连乘，如果W中有其中一个特征值大于1，则多次累乘之后的结果将非常大，自然就产生了梯度爆炸的问题。</p><p><strong>那为什么ReLU在CNN中不存在连乘的梯度爆炸问题呢？</strong>因为在CNN中，每一层都有不同的参数$W_i$，有的特征值大于1，有的小于1，在某种意义上可以理解为抵消了梯度爆炸的可能。</p><h3 id="如何解决ReLU神经元“死亡”的问题？"><a href="#如何解决ReLU神经元“死亡”的问题？" class="headerlink" title="如何解决ReLU神经元“死亡”的问题？"></a>如何解决ReLU神经元“死亡”的问题？</h3><p>①采用Leaky ReLU等激活函数    ②设置较小的学习率进行训练    ③使用momentum优化算法动态调整学习率</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://blog.csdn.net/neo_lcx/article/details/100122938">最全最详细的常见激活函数总结（sigmoid、Tanh、ReLU等）及激活函数面试常见问题总结</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2022/12/05/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2022/12/05/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<p>这是一篇测试文章</p><p><img src="%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/test.png" alt="图片引用方法二"></p>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/12/05/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/hello-world/"/>
    <url>/2022/12/05/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>19-正则化深入理解</title>
    <link href="/2022/11/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/19_%E6%AD%A3%E5%88%99%E5%8C%96%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/"/>
    <url>/2022/11/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/19_%E6%AD%A3%E5%88%99%E5%8C%96%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="为什么需要正则化"><a href="#为什么需要正则化" class="headerlink" title="为什么需要正则化"></a>为什么需要正则化</h2><p>深度学习可能存在过拟合问题，有两个解决方法：1.正则化  2.准备更多的数据 我们无法时刻准备充足的样本数据，因为获取数据本身成本就很高，但正则化技术通常有助于避免过拟合或减少网络参数。</p><h2 id="常见的正则化技术"><a href="#常见的正则化技术" class="headerlink" title="常见的正则化技术"></a>常见的正则化技术</h2><p>常见的机器学习算法的正则化方法有：L1正则化、L2正则化、Dropout、Early Stop、归一化（BN、IN、GN、LN）等</p><h3 id="L1和L2正则化"><a href="#L1和L2正则化" class="headerlink" title="L1和L2正则化"></a>L1和L2正则化</h3><p><strong>定义</strong></p><p>正则化就是在原来损失函数的基础上加了一些正则化项（也叫模型复杂度惩罚项）。即目标函数变成了<strong>原始损失函数+额外项</strong></p><p>L1正则化和L2正则化可以看做是<strong>损失函数的惩罚项</strong>。所谓<strong>惩罚</strong>是指对损失函数中的<strong>某些参数做一些限制</strong>。对于线性回归模型，<strong>使用L1正则化的模型叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）</strong>。</p><p>线性回归L1正则化损失函数：</p><p><img src="C:\Users\10428\AppData\Roaming\Typora\typora-user-images\image-20221101103546867.png" alt="image-20221101103546867"></p><p>线性回归L2正则化损失函数：</p><p><img src="C:\Users\10428\AppData\Roaming\Typora\typora-user-images\image-20221101103632992.png" alt="image-20221101103632992"></p><ul><li>公式(1)(2)中ww表示特征的系数（xx的参数），可以看到正则化项是对系数做了限制。L1正则化和L2正则化的说明如下：<ul><li>L1正则化是指权值向量ww中各个元素的绝对值之和，通常表示为∥w∥1‖w‖1。</li><li>L2正则化是指权值向量ww中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为∥w∥22‖w‖22。</li><li>一般都会在正则化项之前添加一个系数λλ。Python中用α表示，这个系数需要用户指定（也就是我们要调的超参）。</li></ul></li></ul><p><strong>作用</strong></p><ul><li><p><strong>L1正则化可以使得参数稀疏化，即得到的参数是一个稀疏矩阵，可以用于特征选择。</strong></p><p><strong>稀疏性</strong>，说白了就是模型的很多参数是0。通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，很多参数是0，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，即使去掉对模型也没有什么影响，此时我们就可以只关注系数是非零值的特征。这相当于对模型进行了一次特征选择，只留下一些比较重要的特征，提高模型的泛化能力，降低过拟合的可能。</p></li><li><p><strong>L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合。</strong></p><p><strong>拟合过程中通常都倾向于让权值尽可能小</strong>，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。<strong>可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响</strong>，专业一点的说法是<strong>抗扰动能力强</strong>。</p></li></ul><p><strong>L1, L2参数如何选择</strong></p><p>以L2正则化参数为例：从公式(8)可以看到，λ越大，θjθj衰减得越快。另一个理解可以参考L2求解图， <strong>λλ越大，L2圆的半径越小，最后求得代价函数最值时各参数也会变得很小</strong>；当然也不是越大越好，太大容易引起欠拟合。</p><p>从0开始，逐渐增大λλ。在训练集上学习到参数，然后在测试集上验证误差。反复进行这个过程，直到测试集上的误差最小。一般的说，随着λλ从0开始增大，测试集的误分类率应该是先减小后增大，交叉验证的目的，就是为了找到误分类率最小的那个位置。建议一开始将正则项系数λ设置为0，先确定一个比较好的learning rate。然后固定该learning rate，给λλ一个值（比如1.0），然后根据validation accuracy，将λ增大或者减小10倍，增减10倍是粗调节，当你确定了λλ的合适的数量级后，比如λ=0.01λ=0.01，再进一步地细调节，比如调节为0.02，0.03，0.009之类。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/zingp/p/10375691.html">深入理解L1、L2正则化</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>17-各种卷积操作串讲</title>
    <link href="/2022/11/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/17_%E5%90%84%E7%A7%8D%E5%8D%B7%E7%A7%AF%E6%96%B9%E5%BC%8F%E4%B8%B2%E8%AE%B2/"/>
    <url>/2022/11/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/17_%E5%90%84%E7%A7%8D%E5%8D%B7%E7%A7%AF%E6%96%B9%E5%BC%8F%E4%B8%B2%E8%AE%B2/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>CNN 从 2012 年的 AlexNet 发展至今，各种网络结构层出不穷，尝试了几乎所有可能性的结构搭配以试图找到效果更好的那种，再通过结果去解释过程，这大概就是做深度学习的人的无奈之处吧，每天都有新论文发出，每天都会有新的网络结果，每个都比之前的提升一丢丢，琳琅满目，令人眼花缭乱，像我这样的便迷失其中，找不到一个确定的方向去研究，终究普普通通，无所建树。</p><p>网络结构如此，卷积 (Convolution) 方式也不例外，各种可能性的卷积过程改变方式都出现了（以后大概还有出现新的卷积方式的），效果各异，特点不同，所以想通过这篇文章将各种卷积方式捋一捋，体会下大牛们的智慧！</p><h2 id="符号约定"><a href="#符号约定" class="headerlink" title="符号约定"></a>符号约定</h2><p><strong>输入</strong>：$H_{in}*W_{in}*C_{in}$ ，其中 $H_{in}$ 为输入 feature map 的高，$W_{in}$ 为宽，$C_{in}$ 为通道数</p><p><strong>输出</strong>：$H_{out}*W_{out}*C_{out}$ ，其中 $H_{out}$ 为输出 feature map 的高，$W_{out}$ 为宽，$C_{out}$ 为通道数</p><p><strong>卷积核</strong>：$N<em>K</em>K*C_k$ ，其中 $N$ 为该卷积层的卷积核个数，$K$ 为卷积核宽与高(默认相等)，$C_k$ 为卷积核通道数</p><p>这里我们先不考虑卷积步长 stride 和 padding 等，这些只影响输入 feature map 的大小。且假定你已熟悉普通意义下的卷积操作。</p><h2 id="一、常规卷积-Convolution"><a href="#一、常规卷积-Convolution" class="headerlink" title="一、常规卷积(Convolution)"></a>一、常规卷积(Convolution)</h2><p><strong>特点：</strong></p><ol><li>卷积核通道数与输入 feature map 的通道数相等，即 $C_{in} = C_k$</li><li>输出 feature map 的通道数等于卷积核的个数，即 $C_{out} = N$</li></ol><p><strong>卷积过程：</strong></p><p>卷积核在输入 feature map 中移动，按位点乘后求和即可，卷积的实现过程参考之前写过的一篇文章(03_代码实现卷及操作)。</p><p>下面是一个卷积动图，帮助你理解。</p><p><img src="https://i.loli.net/2020/05/20/zUpiqr4LhTOsJCc.gif" alt="img"></p><h2 id="二、1x1卷积"><a href="#二、1x1卷积" class="headerlink" title="二、1ｘ1卷积"></a>二、1ｘ1卷积</h2><p><strong>特点：</strong></p><ol><li>顾名思义，卷积核大小为 1ｘ1</li><li>卷积核通道数与输入 feature map 的通道数相等，即 $C_{in} = C_k$</li><li>输出 feature map 的通道数等于卷积核的个数，即 $C_{out} = N$</li><li>不改变 feature map 的大小，目的是为了改变 channel 数，即 1ｘ1 卷积的使用场景是：<strong>不想改变输入 feature map 的宽高，但想改变它的通道数</strong>。即可以用于升维或降维。</li><li>相比 3ｘ3 等卷积，计算量及参数量都更小，计算量和参数量的计算参考另一篇文章 (22_CNN网络各种层的FLOPs和参数量paras计算)</li></ol><p><strong>作用：</strong></p><ol><li><strong>升维或降维</strong>。改变 feature map 的大小，目的是为了改变 channel 数，即 1ｘ1 卷积的使用场景是：<strong>不想改变输入 feature map 的宽高，但想改变它的通道数</strong></li><li><strong>加入非线性</strong>。1*1卷积核，可以在保持feature map尺度不变的（即不损失分辨率）的前提下大幅增加非线性特性（利用后接的<a href="https://www.zhihu.com/search?q=%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:369745892%7D">非线性激活函数</a>），把网络做的很deep。</li><li><strong>跨通道信息交互（channal 的变换）。</strong>实现降维和升维的操作其实就是channel间信息的<a href="https://www.zhihu.com/search?q=%E7%BA%BF%E6%80%A7%E7%BB%84%E5%90%88&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:369745892%7D">线性组合</a>变化</li></ol><p><strong>示意图：</strong></p><p><img src="https://i.loli.net/2020/05/20/6PHNhYlaXf2KOCt.png" alt="img"></p><p>(图来自这个<a href="https://www.cnblogs.com/lemonzhang/p/9939187.html">博客</a>)</p><p>1x1核的主要目的是应用非线性。在神经网络的每一层之后，我们都可以应用一个激活层。无论是ReLU、PReLU、Sigmoid还是其他，与卷积层不同，激活层是非线性的。非线性层扩展了模型的可能性，这也是通常使“深度”网络优于“宽”网络的原因。为了在不显著增加参数和计算量的情况下增加非线性层的数量，我们可以应用一个1x1内核并在它之后添加一个激活层。这有助于给网络增加一层深度。 </p><h2 id="三、分组卷积-Group-convolution"><a href="#三、分组卷积-Group-convolution" class="headerlink" title="三、分组卷积(Group convolution)"></a>三、分组卷积(Group convolution)</h2><p>Group convolution 分组卷积，最早在 AlexNet 中出现，由于当时的硬件资源有限，训练 AlexNet 时卷积操作不能全部放在同一个 GPU 处理，因此作者把 feature maps 分给多个GPU分别进行处理，最后把多个 GPU 的结果进行融合。</p><p><strong>卷积过程：</strong></p><p>将输入 feature map 分成 g 组，一个卷积核也相对应地分成 g 组，在对应的组内做卷积。（我们可以理解成分组卷积中使用的 g 组卷积核整体对应于常规卷积中的一个卷积核，只不过是将常规卷积中的一个卷积核分成了 g 组而已）</p><p><img src="https://i.loli.net/2020/05/20/KUi1wcMQr38AHel.png" alt="img"></p><p>(图来自这个<a href="https://www.jianshu.com/p/a936b7bc54e3">博客</a>)</p><p><strong>特点、作用：</strong></p><ol><li>输入的 feature map 尺寸：$H_{in}<em>W_{in}</em> \frac{C_{in}}{g}$ ，共有 g 组</li><li>卷积核的规格：$N<em>K</em>K*\frac{C_{k}}{g}$，共有 N * g 组</li><li>输出 feature map 规格：$H_{out}<em>W_{out}<em>N</em>g$ ，共生成 N</em>g 个 feature map</li><li>当 $g = 1$ 时就退化成了上面讲过的常规卷积，当 $g = C_{in}$ 时就是我们下面将要讲述的深度分离卷积。</li><li>用常规卷积得到一个输出 feature map 的计算量和参数量便可以得到 g 个输出 feature map，所以<strong>分组卷积常用在轻量型高效网络中，因为它可以用少量的参数量和计算量生成大量的 feature map。</strong></li></ol><h2 id="四、可分离卷积"><a href="#四、可分离卷积" class="headerlink" title="四、可分离卷积"></a>四、可分离卷积</h2><p>可分离卷积又分成两种：<strong>空间可分离卷积</strong> 和 <strong>深度可分离卷积</strong>。</p><h3 id="空间可分离卷积"><a href="#空间可分离卷积" class="headerlink" title="空间可分离卷积"></a>空间可分离卷积</h3><p>之所以命名为空间可分离卷积，是因为它主要处理的是卷积核的空间维度：宽度和高度。</p><p>空间可分离卷积简单地将卷积核划分为两个较小的卷积核。 最常见的情况是将3x3的卷积核划分为3x1和1x3的卷积核，如下所示：</p><p><img src="https://i.loli.net/2020/05/20/9MGJZqsSBfVKvm7.jpg" alt="img"></p><p><img src="https://i.loli.net/2020/05/20/sieXOFqH2tf7RNd.jpg" alt="img"></p><p><strong>特点：</strong></p><ol><li><strong>局限性</strong>：并不是所有的卷积核都可以“分离”成两个较小的卷积核，<strong>能够“分离”的是那些卷积核参数大小的行和列有一定倍数关系的</strong>. 这在训练期间变得特别麻烦，因为网络可能采用所有可能的卷积核，它最终只能使用可以分成两个较小卷积核的一小部分。所以实际中用的不多</li><li><strong>参数量和计算量更少</strong>：如上图所示，不是用9次乘法进行一次卷积，而是进行两次卷积，每次3次乘法（总共6次），以达到相同的效果。 乘法较少，计算复杂性下降，网络运行速度更快。</li></ol><h3 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h3><p>与空间可分离卷积不同，深度可分离卷积与卷积核无法“分解”成两个较小的内核。 因此，它更常用。 这是在keras.layers.SeparableConv2D或tf.layers.separable_conv2d中看到的可分离卷积的类型。</p><p>之所以命名为深度可分离卷积，是因为第一步卷积的时候是通道独立的（后面会看到这种卷积方式分成两步），你可以将每个通道想象成对该图像特定的解释说明（interpret）; 例如RGB图像中，“R”通道解释每个像素的“红色”，“B”通道解释每个像素的“蓝色”，“G”通道解释每个像素的“绿色”。 有多少个通道就有多少种解释。</p><p><strong>卷积过程：</strong></p><p>深度可分离卷积的过程分为两部分：<strong>深度卷积</strong>(depthwise convolution) 和 <strong>逐点卷积</strong>(pointwise convolution)</p><p>(1) 深度卷积</p><p>深度卷积意在保持输入 feature map 的通道数，即对 feature map 中的每个通道使用一个规格为 $K<em>K</em>1$ 的卷积核进行卷积，于是输入 feature map 有多少个通道就有多少个这样的卷积核，深度卷积结束后得到的输出的通道数与输入的相等。</p><p>这一步其实就相当于常规卷积中的一个卷积核，只不过不同通道的卷积结果不相加而已，自己体会体会。</p><p><img src="https://i.loli.net/2020/05/20/Y13jLQXx2coWyzk.png" alt="img"></p><p>(2) 逐点卷积</p><p>在上一步的基础上，运用 1ｘ1 卷积进行逐点卷积。</p><p>使用一个  1ｘ1 卷积核就可以得到输出 feature map 一维的结果。</p><p><img src="https://i.loli.net/2020/05/20/9BxLoJkTcj2tPhD.png" alt="img"></p><p>如果你要输出 feature map 有 256 维，那么就使用 256 个  1ｘ1 卷积核即可。</p><p><img src="https://i.loli.net/2020/05/20/Z7MbCnfr3NJDw64.png" alt="img"></p><p><strong>特点：</strong></p><ol><li>可以理解成常规的卷积分成了两步执行，<strong>但是分成两步后参数量和计算量大大减少</strong>，网络运行更快</li></ol><h2 id="转置卷积-transposed-convolution"><a href="#转置卷积-transposed-convolution" class="headerlink" title="转置卷积  (transposed convolution)"></a>转置卷积  (transposed convolution)</h2><p><strong>转置卷积常常用于 CNN 中对特征图进行上采样</strong>，比如语义分割和超分辨率任务中。之所以叫转置卷积是因为，它其实是把我们平时所用普通卷积操作中的卷积核做一个转置，然后把普通卷积的输出作为转置卷积的输入，而转置卷积的输出，就是普通卷积的输入。这样说可能有点绕，我们可以参照CNN中的反向传播过程来理解，转置卷积形式上就和一个卷积层的反向梯度计算相同。既然是输入输出对调，那么就有两个很重要的特性：</p><ol><li>转置的卷积核变为了普通卷积核的转置</li><li>如果把由输入特征图到输出特征图的计算过程画成一个计算图，那么输入输出元素的连接关系是不变的.也就是说，在普通卷积中，若元素a和元素1有连接（元素1由a计算得到），那么在相应的转置卷积中，元素1和元素a依然是有连接的（元素a由元素1计算得到）。</li></ol><p>下面通过对比常规卷积和转置卷积的计算过程帮助理解：</p><p><strong>常规卷积：</strong></p><p><img src="https://i.loli.net/2020/05/20/oMn6BrWRQhi231w.png" alt="这里写图片描述"></p><p>这是一个卷积核大小为3x3，步长为2，padding为1的普通卷积。卷积核在红框位置时输出元素1，在绿色位置时输出元素2。我们可以发现，输入元素a仅和一个输出元素有运算关系，也就是元素1，而输入元素b和输出元素1, 2均有关系。同理c只和一个元素2有关，而d和1,2,3,4四个元素都有关。那么在进行转置卷积时，依然应该保持这个连接关系不变。</p><p><strong>转置卷积：</strong></p><p>根据前面的分析，我们需要将上图中绿色的特征图作为输入，蓝色的特征图作为输出，并且保证连接关系不变。也就是说，a只和1有关，b和1,2两个元素有关，其它类推。怎么才能达到这个效果呢？我们可以先用0给绿色特征图做插值，插值的个数就是使相邻两个绿色元素的间隔为卷积的步长，同时边缘也需要进行与插值数量相等的补0。如下图：</p><p><img src="https://i.loli.net/2020/05/20/eb9JmN3oRSt7qnx.png" alt="这里写图片描述"></p><p>注意，这时候卷积核的滑动步长就不是2了，而是1，步长体现在了插值补0的过程中。</p><p><strong>输出特征图的尺寸计算：</strong></p><p>假设我们做转置卷积的输入特征图大小为 $n<em>n$，卷积核大小为 $k</em>k$，后面为了表示方便，我们直接使用边长来表示大小。步长stride为 s（注意这个步长是前面做卷积的时候的步长，设计网络结构的时候对应过来即可），那么转置卷积需要在四周每个边缘补0的数量为 s-1，边缘和内部插空补0后输入特征图大小变为：<br>$$<br>0插值后的输入特征图大小：s<em>n+s-1<br>$$<br>使用大小为 k 的卷积核进行卷积（滑动步长为1），得到的输出特征图大小为：<br>$$<br>\frac{s</em>n+s-1-k}{1}+1 = s*n+(s-k)<br>$$<br>当然这个公式只是转置卷积的一种理解方法，在实际的实现中，还有不同的padding, stride和dilation配置，输出图像大小也会随之改变。</p><p><strong>可能疑惑的地方：</strong></p><ol><li>为什么人们很喜欢叫<strong>转置卷积为反卷积或逆卷积</strong>。首先举一个例子，将一个4x4的输入通过3x3的卷积核在进行普通卷积（无padding, stride=1），将得到一个2x2的输出。而转置卷积将一个2x2的输入通过同样3x3大小的卷积核将得到一个4x4的输出，看起来似乎是普通卷积的逆过程。就好像是加法的逆过程是减法，乘法的逆过程是除法一样，人们自然而然的认为这两个操作似乎是一个可逆的过程。<strong>但事实上两者并没有什么关系，操作的过程也不是可逆的</strong>。具体对转置卷积名称的理解可参考这篇<a href="https://blog.csdn.net/tsyccnh/article/details/87357447">博客</a>。</li></ol><p>转置卷积蛮复杂的，很多博客讲解的角度都不太一样，看得有点晕，这里先放一放，后面有心情了再好好研究下……</p><h2 id="空洞卷积-Dilated-Atrous-Convolutions"><a href="#空洞卷积-Dilated-Atrous-Convolutions" class="headerlink" title="空洞卷积  (Dilated / Atrous Convolutions)"></a>空洞卷积  (Dilated / Atrous Convolutions)</h2><p>空洞卷积诞生在图像分割领域，在一般的卷积结构中因为存在 pooling 操作，<strong>目的是增大感受野也增加非线性等</strong>，但是 pooling 之后特征图的大小减半，而图像分割是 pixelwise 的，因此后续需要 upsamplng 将变小的特征图恢复到原始大小，这里的 upsampling 主要是通过转置卷积完成，但是经过这么多的操作之后会将很多细节丢失，那么空洞卷积就是来解决这个的，既扩大了感受野，又不用 pooling 。</p><p>Dilated/Atrous Convolution 或者是 Convolution with holes 从字面上就很好理解，是在标准的 convolution map 里注入空洞，以此来增加感受野。相比原来的正常卷积，空洞多了一个 超参数，称之为 dilation rate 指的是kernel的间隔数量(e.g. 正常的卷积是 dilatation rate 1)。</p><p>在VGG网络中就证明了使用小卷积核叠加来取代大卷积核可以起到减少参数同时达到大卷积核同样大小感受野的功效。但是通过叠加小卷积核来扩大感受野只能线性增长，公式为$(kernelSize - 1)*layers+1$,，也就是线性增长，而空洞卷积可以以指数级增长感受野。</p><p>先来感受下空洞卷积和常规卷积的不同之处：Dilated Convolution with a 3 x 3 kernel and dilation rate 2</p><p><img src="https://i.loli.net/2020/05/20/s7L9FWCutVchJ3y.gif" alt="这里写图片描述"></p><p>要理解空洞概念和如何操作可以从两个角度去看。</p><ol><li>从原图角度，所谓空洞就是在原图上做采样。采样的频率是根据rate参数来设置的，当rate为1时候，就是原图不丢失任何信息采样，此时卷积操作就是标准的卷积操作，当rate&gt;1，比如2的时候，就是在原图上每隔一（rate-1）个像素采样，如图b，可以把红色的点想象成在原图上的采样点，然后将采样后的图像与kernel做卷积，这样做其实变相增大了感受野。</li><li>从kernel角度去看空洞的话就是扩大kernel的尺寸，在kernel中，相邻点之间插入rate-1个零，然后将扩大的kernel和原图做卷积 ，这样还是增大了感受野。</li></ol><p>在语义分割任务中，当它与双线性插值一起使用时，可以替代转置卷积。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.jianshu.com/p/2a0f3a4a9d1d">卷积网络CNN中各种常见卷积过程</a><br><a href="https://baijiahao.baidu.com/s?id=1634399239921135758&wfr=spider&for=pc">可分离卷积基本介绍</a><br><a href="https://blog.csdn.net/hongxingabc/article/details/79563525">CNN中卷积操作十大改进方向</a><br><a href="https://blog.csdn.net/isMarvellous/article/details/80087705">关于转置卷积（反卷积）的理解</a><br><a href="https://blog.csdn.net/silence2015/article/details/79748729">Dilated/Atrous conv 空洞卷积</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>18—concat与add区别</title>
    <link href="/2022/11/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/18_%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88concat%E5%92%8Cadd%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <url>/2022/11/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/18_%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88concat%E5%92%8Cadd%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在网络模型当中，经常要进行不同通道特征图的信息融合相加操作，以整合不同通道的信息，在具体实现方面特征的融合方式一共有两种，一种是 ResNet 和 FPN 等当中采用的 element-wise add ，另一种是 DenseNet 等中采用的 concat 。他们之间有什么区别呢？</p><h2 id="add"><a href="#add" class="headerlink" title="add"></a>add</h2><p>举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a = [[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br>b =  [[<span class="hljs-number">11</span>,<span class="hljs-number">12</span>], [<span class="hljs-number">13</span>, <span class="hljs-number">14</span>]]<br>c = add(a, b)  <span class="hljs-comment"># c = [[12,14], [16, 18]]  这里add表示add层操作，把输出结果值相加了</span><br></code></pre></td></tr></table></figure><p>从中可以很容易地看出，add 方式有以下特点：</p><ol><li>做的是对应通道对应位置的值的相加，<strong>通道数不变</strong></li><li>描述图像的特征个数不变，但是每个特征下的信息却增加了。</li></ol><h2 id="concat"><a href="#concat" class="headerlink" title="concat"></a>concat</h2><p>也举个例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a = [[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br>b =  [[<span class="hljs-number">11</span>,<span class="hljs-number">12</span>], [<span class="hljs-number">13</span>, <span class="hljs-number">14</span>]]<br>c = concat(a, b)  <span class="hljs-comment"># c = [[1,2], [3, 4], [11,12], [13, 14]]  这里concat表示concat层操作，把输出结果级联，增加了维度</span><br></code></pre></td></tr></table></figure><p>从中可以很容易地看出，concat 方式有以下特点：</p><ol><li>做的是<strong>通道的合并</strong>，通道数变多了</li><li>描述图像的特征个数变多，但是每个特征下的信息却不变。</li></ol><h2 id="多一点理解"><a href="#多一点理解" class="headerlink" title="多一点理解"></a>多一点理解</h2><p>DenseNet和Inception中更多采用的是concatenate操作，而ResNet更多采用的add操作，Resnet是做值的叠加，通道数是不变的，DenseNet是做通道的合并。你可以这么理解，add是描述图像的特征下的信息量增多了，但是描述图像的维度本身并没有增加，只是每一维下的信息量在增加，这显然是对最终的图像的分类是有益的。而concatenate是通道数的合并，也就是说描述图像本身的特征增加了，而每一特征下的信息是没有增加。</p><blockquote><p>add相当于加了一种prior，当两路输入可以具有“对应通道的特征图语义类似”的性质的时候，可以用add来替代concat，这样更节省参数和计算量（concat是add的2倍）</p></blockquote><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://blog.csdn.net/qq_32256033/article/details/89516738">理解concat和add的不同作用</a></p><p><a href="https://blog.csdn.net/weixin_39610043/article/details/87103358">卷积神经网络中的add和concatnate区别</a></p><p><a href="https://www.i4k.xyz/article/qq_26369907/89351328">ShuffleNet中add层和concatenate层的区别</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>15-CNN分类网络发展历程</title>
    <link href="/2022/11/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/15_%E7%AE%80%E8%BF%B0CNN%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BC%94%E5%8F%98%E8%84%89%E7%BB%9C%E5%8F%8A%E5%90%84%E8%87%AA%E7%9A%84%E8%B4%A1%E7%8C%AE%E4%B8%8E%E7%89%B9%E7%82%B9/"/>
    <url>/2022/11/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/15_%E7%AE%80%E8%BF%B0CNN%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BC%94%E5%8F%98%E8%84%89%E7%BB%9C%E5%8F%8A%E5%90%84%E8%87%AA%E7%9A%84%E8%B4%A1%E7%8C%AE%E4%B8%8E%E7%89%B9%E7%82%B9/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>简述CNN分类网络的演变脉络及各自的贡献与特点</p><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>深度学习的浪潮就是从CNN开始的，它结构形态的变化也见证着这门技术的发展。现在涌进来学习深度学习的大部分人都是做计算机视觉的，因为这个门槛相对较低，业界数据集开源了很多，也比较直观，而且对硬件设备的要求也没以前那么大，导致现在就业竞争非常大。CV各种任务的网络结构变形更是日新月异，让人眼花缭乱，但是不管怎么变，基本都是基于卷积、池化和全连接这三件套。所以这篇文章主要是想回归一下最初CNN网络是怎么发展变化的。</p><h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><p>LeNet 被视为CNN的开山之作，是LeCun大神在1998年提出的，<strong>定义了基本组件：卷积、池化、全连接，俗称CPF三件套</strong>。主要用于手写数字识别10分类问题，被广泛应用在金融领域。LeNet的结构非常简单，如名字所示的那样，主要有5层（这里没有统计紧跟每个卷积层后的 2x2 池化层，如果加上池化层，应该是7层），包括：<strong>两层 5x5 卷积层和三层全连接层</strong>。下面便是LeNet-5架构示意图。</p><p><img src="https://pic1.zhimg.com/80/v2-ee8b9b069a5e4e0d906ea955e9ae4e40_720w.jpg" alt="img"></p><h3 id="LeNet-的关键点与贡献："><a href="#LeNet-的关键点与贡献：" class="headerlink" title="LeNet 的关键点与贡献："></a>LeNet 的关键点与贡献：</h3><ol><li>LeNet 是第一个将反向传播应用于实际应用的CNN架构，突然之间，深度学习不再只是一种理论。</li><li>定义了基本组件：卷积、池化、全连接。</li></ol><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>可以说深度学习浪潮是从这个网络开始的，它是 ImageNet 图片分类竞赛 2012 年的冠军。AlexNet 有着和 LeNet-5 相似网络结构，但更深、有更多参数。<strong>AlexNet包含5层卷积和3层全连接，实现1000分类</strong>。（注意：AlexNet 采用的 是11x11，7x7与5x5大小的卷积核）</p><p>有意思的是，在当时由于算力不足，作者将网络分上下两部分分别卷积，放在两块GPU上进行训练。这一操作倒是给了后来人的诸多启发，典型的如 shuffle-net，借用该想法进行轻量级改进。</p><img src="https://i.loli.net/2020/06/26/9VbjCshgR1Ypxk2.png" alt="https://upload-images.jianshu.io/upload_images/7229096-47de76410f44a0af.png" style="zoom:80%;" /><h3 id="AlexNet-关键点与贡献："><a href="#AlexNet-关键点与贡献：" class="headerlink" title="AlexNet 关键点与贡献："></a>AlexNet 关键点与贡献：</h3><ol><li>如上所述的<strong>多 GPU 训练技术</strong>，不同的通道放在不同GPU上运算，最后再cat起来，是一种工程技术实现。</li><li>使用了 <strong>ReLU</strong> 激活函数，使之有更好的梯度特性、训练更快，后来发现和其他激活函数也差不多，只是因为比较简单。</li><li>使用了**随机失活(dropout)**。对于全连接层以及后来RNN，Atttention 挺有用的，但对于全卷积来说没啥大用。</li><li>使用<strong>数据扩充</strong>技术。1.随机裁切 2.改变颜色通道</li></ol><h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><p>这是 ImageNet 2014年的亚军网络。当然 ImageNet 2013也举办了竞赛，但是其中的 ZF-Net 也就只是调整AlexNet的参数，包括卷积核大小、卷积步长、通道数，没啥亮点可言。</p><p>VGG 主要在网络深度上进行探索，提出了两个深度的网络 <strong>VGG16（13 层卷积 + 3 层全连接）</strong>和 VGG19（16 层卷积 + 3 层全连接），由于VGG-16网络结构十分简单，并且很适合迁移学习，因此至今VGG-16仍在广泛使用。</p><p><img src="https://i.loli.net/2020/06/26/BIaJfzVQTbdguXh.jpg" alt="preview"></p><h3 id="VGG-关键点与贡献："><a href="#VGG-关键点与贡献：" class="headerlink" title="VGG 关键点与贡献："></a>VGG 关键点与贡献：</h3><ol><li><p><strong>结构简单</strong>，只有 3×3 卷积和2×2 汇合两种配置，并且<strong>重复堆叠</strong>相同的模块组合。卷积层不改变空间大小，每经过一次汇合层，空间大小减半。</p><p>VGG提出了将所有这些卷积核分解为 3x3 卷积核的想法，按照感受野的概念去理解的话，两层 3x3 的卷积层的感受野与一层 5x5 的卷积层感受野一样，三层 3x3 的卷积层的感受野与一层 7x7 的卷积层感受野一样。但是 <strong>3x3 卷积层的堆叠可以在获得相同感受野的前提下使用更少的参数，多层的卷积也能增加模型的非线性效果。</strong></p></li><li><p><strong>初始化方法：先训练浅层网络，并使用浅层网络对深层网络进行初始化。</strong>（这是在 BN 层没提出来之前的很好的一种模型训练方法）。</p></li></ol><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p>GoogLeNet 是 ImageNet 2014的冠军网络。GoogLeNet 不仅在深度上进行探索，还增加了网络的宽度，试图回答在设计网络时究竟应该选多大尺寸的卷积、或者应该选汇合层。其提出了Inception模块，同时用1×1、3×3、5×5卷积和3×3汇合相同尺寸不同深度的feature map，并concat所有结果。详情点这：<a href="https://blog.csdn.net/csdn_xmj/article/details/116604126">inception v1网络</a>   <a href="https://zhuanlan.zhihu.com/p/30756181">inception v1-v4</a></p><p><img src="https://i.loli.net/2020/06/26/Xe5pz8TAjJdVW9k.jpg" alt="preview"></p><h3 id="GoogLeNet-关键点与贡献："><a href="#GoogLeNet-关键点与贡献：" class="headerlink" title="GoogLeNet 关键点与贡献："></a>GoogLeNet 关键点与贡献：</h3><ol><li> <strong>多分支</strong>分别处理，并级联结果。</li><li>为了降低计算量，用了<strong>1×1卷积</strong>降维(先降后升)。</li><li>使用了<strong>全局平均池化（Global average pooling）替代全连接层</strong>，使网络参数大幅减少，但目标检测小目标还是全连接好些。</li></ol><p>inception结构在之后的几年中从v1到v4不断改进。</p><blockquote><p><strong>Inception v2</strong>  在 v1 的基础上加入 batch normalization 技术，在tensorflow中，使用 BN 在激活函数之前效果更好；将 5x5 卷积替换成两个连续的 3x3 卷积，使网络更深，参数更少。使用$1<em>n$和  $n</em>1$这种非对称的卷积来代替n*n的对称卷积，既降低网络的参数，又增加了网络的深度</p><p><strong>Inception v3</strong> 核心思想是将卷积核分解成更小的卷积，如将 7x7 分解成 1x7 和 7x1 两个卷积核。</p><p><strong>inception V4</strong> 将原来卷积、池化的顺次连接（网络的前几层）替换为stem模块，来获得更深的网络结，又设计了三种inception模块。</p><p><strong>Inception-ResNet  v2</strong> 在Inception模块基础上结合了 residual 模块。</p></blockquote><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>ResNet 是 ImageNet 2015年的冠军网络，是一个里程碑的事件。ResNet旨在解决网络加深后训练难度增大的现象。其提出了residual模块，包含两个3×3卷积和一个短路连接(左图)。短路连接可以有效缓解反向传播时由于深度过深导致的<strong>梯度消失</strong>现象，这使得网络加深之后性能不会变差。因此 ResNet 网络的层数有 152 之多。</p><h3 id="ResNet核心思想"><a href="#ResNet核心思想" class="headerlink" title="ResNet核心思想"></a>ResNet核心思想</h3><p>引入残差连接，求梯度时加上了浅层网络的梯度，避免了梯度消失，所以能训练更深的网络。</p><img src="C:\Users\10428\AppData\Roaming\Typora\typora-user-images\image-20221102111753223.png" alt="image-20221102111753223" style="zoom:50%;" /><h3 id="ResNet-关键点与贡献："><a href="#ResNet-关键点与贡献：" class="headerlink" title="ResNet 关键点与贡献："></a>ResNet 关键点与贡献：</h3><ol><li><p>使用<strong>短路连接</strong>，使训练深层网络更容易，并且<strong>重复堆叠</strong>相同的模块组合。</p></li><li><p>ResNet大量使用了<strong>批量归一层</strong>。</p></li><li><p>对于很深的网络(超过50层)，ResNet使用了更高效的**<a href="https://zhuanlan.zhihu.com/p/98692254">瓶颈(bottleneck)</a>**结构(右图)。</p><p><img src="https://i.loli.net/2020/06/26/IRrt4mA1uUHpb7E.jpg" alt="preview"></p></li></ol><h2 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a>ResNeXt</h2><p>ResNeXt 是 ImageNet 2016年的亚军网络，是 ResNet 的一个改进。传统的方法通常是靠加深或加宽网络来提升性能，但计算开销也会随之增加。ResNeXt旨在不改变模型复杂度的情况下提升性能。受精简而高效的Inception模块启发，ResNeXt将ResNet中非短路那一分支变为多个分支。</p><img src="https://i.loli.net/2020/06/26/MtHLK2zl41Co63O.png" alt="preview" style="zoom:80%;" /><h3 id="ResNeXt-关键点与贡献："><a href="#ResNeXt-关键点与贡献：" class="headerlink" title="ResNeXt 关键点与贡献："></a>ResNeXt 关键点与贡献：</h3><ol><li>在 ResNet 的短路连接基础上，综合了 Inception 的优点，使用多分支进行处理，但是与 Inception不同的是，其<strong>每个分支的结构都相同</strong>。</li><li>ResNeXt巧妙地利用<strong>分组卷积</strong>进行实现。</li></ol><h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p><strong>DenseNet</strong>  其目的也是<u>避免梯度消失。</u>和residual模块不同，dense模块中任意两层之间均有短路连接。也就是说，每一层的输入通过级联(concatenation)包含了之前所有层的结果，即包含由低到高所有层次的特征。和之前方法不同的是，DenseNet中卷积层的滤波器数很少。DenseNet只用ResNet一半的参数即可达到ResNet的性能。</p><p>实现方面，作者在大会报告指出，直接将输出级联会占用很大GPU存储。后来，通过共享存储，可以在相同的GPU存储资源下训练更深的DenseNet。但由于有些中间结果需要重复计算，该实现会增加训练时间。 </p><p><img src="https://i.loli.net/2020/06/26/7D6JqyXYgoHQ2TA.png" alt="preview"></p><h2 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a>SENet</h2><p>SENet 是 ImageNet 2017年的冠军网络，也是 ImageNet 竞赛的收官之作。SENet 通过额外的分支(gap-fc-fc-sigm)来得到<strong>每个通道的[0, 1]权重</strong>，自适应地校正原各通道激活值响应。以提升有用的通道响应并抑制对当前任务用处不大的通道响应。</p><p>这其实是一种<strong>通道注意力机制</strong>，因为不是每个通道的信息对结果都是同等重要的。</p><img src="https://i.loli.net/2020/06/26/IzyaK6h785upLfs.jpg" alt="preview" style="zoom:60%;" /><h2 id="MobileNet-和-ShuffleNet-轻量化网络"><a href="#MobileNet-和-ShuffleNet-轻量化网络" class="headerlink" title="MobileNet 和 ShuffleNet 轻量化网络"></a>MobileNet 和 ShuffleNet 轻量化网络</h2><p>在实际中应用，CNN受限于硬件运算能力与存储（比如几乎不可能在ARM芯片上跑ResNet-152）。<strong>所以必须有一种能在算法层面有效的压缩存储和计算量的方法。而MobileNet/ShuffleNet正为我们打开这扇窗。</strong></p><p>谈论起MoblieNet / ShuffleNet这些网络结构，就绕不开<strong>分组卷积 Group convolution</strong>，甚至可以认为这些网络结构只是Group convolution的变形而已。（不了解分组卷积的请移步到问题 “ 48_各种卷积方式串讲.md “）。<u>分组卷积可以大大减少参数量和计算量</u>。</p><p>其实我们上面在讲 AlexNet 的时候说到多 GPU 训练，其实便用到了分组卷积这个操作，将整个网络分成了两组。</p><h3 id="MobileNet-v1"><a href="#MobileNet-v1" class="headerlink" title="MobileNet v1"></a>MobileNet v1</h3><p>Mobilenet  v1是Google于2017年发布的网络架构，旨在充分利用移动设备和嵌入式应用的有限的资源，有效地最大化模型的准确性，以满足有限资源下的各种应用案例。Mobilenet v1也可以像其他流行模型（如VGG，ResNet）一样用于分类、检测、嵌入和分割等任务提取图像卷积特征。</p><p>Mobilenet v1核心是把卷积拆分为 Depthwise + Pointwise 两部分。</p><p><strong>Depthwise</strong> 是对 $N<em>C</em>H<em>W$ 的输入进行 group = C 的分组卷积，每一组做 kernel = 3x3，pad = 1，stride = 1 的卷积，因此输出仍然为  $N</em>C<em>H</em>W$ 。这样相当于收集了每个Channel的空间特征，即Depthwise特征。</p><p><strong>Pointwise</strong> 是对 $N<em>C</em>H<em>W$ 的输入进行 k 个普通的 1x1 卷积，收集每个点的特征，最终输出为 $N</em>k<em>H</em>W$ 。</p><blockquote><p>Depthwise只提取通道内部的特征，但是通道与通道间信息是相互隔离的；而Pointwise通过提取每个点的特征，对每个通道间的信息进行交换。Depthwise+Pointwise相当于把普通卷积分为了2个步骤而已。</p></blockquote><p>下面作图为普通的卷积，右图为MobileNet 中的卷积。</p><p><img src="https://i.loli.net/2020/06/26/pSbxWTAD9dhG28R.jpg" alt="preview"></p><p>MobileNet v2 简化来说就是在 v1 基础上引入了 residual 结构，所以就不展开了。</p><h3 id="ShuffleNet-v1"><a href="#ShuffleNet-v1" class="headerlink" title="ShuffleNet v1"></a>ShuffleNet v1</h3><p>ShuffleNet是Face++提出的一种轻量化网络结构，主要思路是使用Group convolution和Channel shuffle改进ResNet，可以看作是ResNet的压缩版本。</p><p>这里再回顾一下ResNet的bottleneck网络结构，如图13。注意Channel维度变化：256D-64D-256D ，宛如一个中间细两端粗的瓶颈，所以称为“bottleneck”。这种结构相比VGG，早已经被证明是非常效的，能够更好的提取图像特征。</p><p>下图展示了ShuffleNet的结构，其中(a)就是加入Depthwise的ResNet bottleneck结构，而(b)和(c)是加入Group convolution和Channel Shuffle的ShuffleNet的结构。</p><p><img src="https://i.loli.net/2020/06/26/MoIS3rdkn7mc2JV.jpg" alt="preview"></p><p>如同上面的 MobileNet ，ShuffleNet 也可以减少参数量和计算量。但为什么要引入Channel Shuffle操作呢？</p><blockquote><p>ShuffleNet的本质是将卷积运算限制在每个Group内，这样模型的计算量取得了显著的下降。然而导致模型的信息流限制在各个Group内，组与组之间没有信息交换，这会影响模型的表示能力。因此，需要引入组间信息交换的机制，即Channel Shuffle操作。同时Channel Shuffle是可导的，可以实现end-to-end一次性训练网络。</p></blockquote><p>下面详细讲解下 channel shuffle 具体是怎么实现的：</p><p><img src="https://i.loli.net/2020/06/26/CBwYdqiphJEKubj.jpg" alt="preview"></p><p>假设输入的 feature map 有 9 个 channels，groups = 3，分成三组：</p><p><img src="https://i.loli.net/2020/06/26/yVIHSDuQmas8YcJ.png"></p><p>第一步进行 reshape：</p><p><img src="https://i.loli.net/2020/06/26/dYhxHE47w8VvDcz.png"></p><p>第二步对得到的矩阵进行矩阵转置操作：</p><p><img src="https://i.loli.net/2020/06/26/iAmkJhR1H3PBfEu.png"></p><p>最后再一次进行reshape操作：</p><p><img src="https://i.loli.net/2020/06/26/rWiAq9ojelUF1fT.png"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/31727402">(二)计算机视觉四大基本任务(分类、定位、检测、分割)</a><br><a href="https://zhuanlan.zhihu.com/p/35405071">轻量化网络ShuffleNet MobileNet v1/v2 解析</a><br><a href="https://blog.csdn.net/baidu_23388287/article/details/94456951">Tensorflow笔记——channel shuffle的实现</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>16—为什么用F1-Score？</title>
    <link href="/2022/11/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/16_%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8F1-score/"/>
    <url>/2022/11/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/16_%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8F1-score/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>为什么使用F1 score？（这里主要讨论为何使用 F1 score 而不是算术平均）</p><h2 id="F1-score"><a href="#F1-score" class="headerlink" title="F1 score"></a>F1 score</h2><p>F1 score是分类问题中常用的评价指标，定义为精确率（Precision）和召回率（Recall）的调和平均数。<br>$$<br>F1=\frac{1}{\frac{1}{Precision}+\frac{1}{Recall}}=\frac{2×Precision×Recall}{Precision+Recall}<br>$$</p><blockquote><p>补充一下精确率和召回率的公式：</p><blockquote><p>TP（ True Positive）：真正例</p><p>FP（ False Positive）：假正例</p><p>FN（False Negative）：假反例</p><p>TN（True Negative）：真反例</p></blockquote><p><strong>精确率（Precision）:</strong>    $Precision=\frac{TP}{TP+FP}$ </p><p><strong>召回率（Recall）:</strong>    $Recall=\frac{TP}{TP+FN}$ </p><blockquote><p>精确率，也称为查准率，衡量的是<strong>预测结果为正例的样本中被正确分类的正例样本的比例</strong>。</p><p>召回率，也称为查全率，衡量的是<strong>真实情况下的所有正样本中被正确分类的正样本的比例。</strong></p></blockquote></blockquote><p>F1 score 综合考虑了精确率和召回率，其结果更偏向于 Precision 和 Recall 中较小的那个，即 Precision 和 Recall 中较小的那个对 F1 score 的结果取决定性作用。例如若 $Precision=1,Recall \approx 0$，由F1 score的计算公式可以看出，此时其结果主要受 Recall 影响。</p><p>如果对 Precision 和 Recall 取算术平均值（$\frac{Precision+Recall}{2}$），对于 $Precision=1,Recall \approx 0$，其结果约为 0.5，而 F1 score 调和平均的结果约为 0。<strong>这也是为什么很多应用场景中会选择使用 F1 score 调和平均值而不是算术平均值的原因，因为我们希望这个结果可以更好地反映模型的性能好坏，而不是直接平均模糊化了 Precision 和 Recall 各自对模型的影响。</strong></p><blockquote><p>补充另外两种评价方法：</p></blockquote><p><strong>加权调和平均：</strong></p><p>上面的 F1 score 中， Precision 和 Recall 是同等重要的，而有的时候可能希望我们的模型更关注其中的某一个指标，这时可以使用加权调和平均：<br>$$<br>F_{\beta}=(1+\beta^{2})\frac{1}{\frac{1}{Precision}+\beta^{2}×\frac{1}{Recall}}=(1+\beta^{2})\frac{Precision×Recall}{\beta^{2}×Precision+Recall}<br>$$<br>当 $\beta &gt; 1$ 时召回率有更大影响， $\beta &lt; 1$ 时精确率有更大影响， $\beta = 1$ 时退化为 F1 score。</p><p><strong>几何平均数：</strong><br>$$<br>G=\sqrt{Precision×Recall}<br>$$</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.cnblogs.com/walter-xh/p/11140715.html">为什么要用f1-score而不是平均值</a> <a href="https://www.cnblogs.com/walter-xh/p/11140715.html">https://www.cnblogs.com/walter-xh/p/11140715.html</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>14-卷积与互相关的联系</title>
    <link href="/2022/11/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/14_%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E4%B8%8E%E4%BA%92%E7%9B%B8%E5%85%B3%E7%9A%84%E9%82%A3%E7%82%B9%E4%BA%8B/"/>
    <url>/2022/11/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/14_%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E4%B8%8E%E4%BA%92%E7%9B%B8%E5%85%B3%E7%9A%84%E9%82%A3%E7%82%B9%E4%BA%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="卷积层的来源与作用"><a href="#卷积层的来源与作用" class="headerlink" title="卷积层的来源与作用"></a>卷积层的来源与作用</h2><p>深度学习的计算机视觉是基于卷积神经网络实现的，卷积神经网络的与传统的神经网络（可以理解为多层感知机）的主要区别是卷积神经网络中除了全连接层外还有卷积层和pooling层等。</p><p>卷积层算是图像处理中非常基础的东西，<strong>它其实也是全连接层演变来的</strong>，卷积可视为<strong>局部连接</strong>和<strong>共享参数</strong>的全连接层。</p><p><strong>局部连接</strong>：在全连接层中，每个输出通过权值(weight)和所有输入相连。而在视觉识别中，关键性的图像特征、边缘、角点等只占据了整张图像的一小部分，图像中相距很远的两个像素之间有相互影响的可能性很小。因此，**<u>在卷积层中，每个输出神经元在通道方向保持全连接，而在空间方向上只和一小部分输入神经元相连。</u>**</p><p><strong>共享参数</strong>： <strong>使用同一组权值去遍历整张图像</strong>，用于发现整张图像中的同一种特征例如角点、边缘等。不同的卷积核用于发现不同的特征。共享参数是深度学习一个重要的思想，其在减少网络参数的同时仍然能保持很高的网络容量(capacity)。<strong>卷积层在空间方向共享参数</strong>，而循环神经网络(recurrent neural networks)在时间方向共享参数。</p><p><strong>卷积层的作用</strong>：通过卷积，我们可以捕获图像的局部信息。通过多层卷积层堆叠，<strong>各层提取到特征逐渐由边缘、纹理、方向等低层级特征过度到文字、车轮、人脸等高层级特征。</strong></p><h2 id="卷积与互相关"><a href="#卷积与互相关" class="headerlink" title="卷积与互相关"></a>卷积与互相关</h2><p>但是其实深度学习各框架的<code>conv2</code>卷积层的API对卷积运算的实现其实使用的是<strong>互相关运算</strong>，即下图：</p><p><img src="https://i.loli.net/2020/05/18/JT9WzRIvteZjuGC.png" alt="在这里插入图片描述"></p><p>上述的运算过程可以写成公式：</p><p><img src="https://i.loli.net/2020/05/18/BkPZM1WwXHDpbQu.png" alt="在这里插入图片描述"></p><p><code>h[u, v]</code>表示filter的权重；<code>k</code>表示neighbor的个数，如当<code>k=1</code>时表示的是<code>3*3</code>的滤波器</p><p>从公式中可以看出这个运算是从上往下，从左到右的对点相乘再相加。所以这个公式代表的运算是互相关。</p><hr><p>下面我们回顾下卷积的定义，设<code>f(x)</code>和<code>g(x)</code>是在R上的可积函数，作积分：</p><p><img src="https://i.loli.net/2020/05/18/16hczmsOuo5fjTK.png" alt="在这里插入图片描述"></p><p>改成离散函数形式如下：</p><p><img src="https://i.loli.net/2020/05/18/QHBcoxKuPyDJ2wj.png" alt="在这里插入图片描述"></p><p>这个公式与互相关的公式很相似，但是注意符号改变了，这就导致运算的方向变成了从下往上，从右到左，与互相关的运算顺序刚好相反。</p><p>所以真正的卷积运算应该是先让卷积核绕自己的核心元素顺时针旋转180度（或者理解为左右翻转再上下翻转），再与图像中的像素做对点相乘再相加运算。</p><p>然而图像处理中的大部分卷积核都是中心对称的，所以这时候的互相关运算与卷积运算结果是一样的，这也许是最开始称为卷积的原因吧。</p><p>另外<code>CNN</code>中的卷积核权值参数是学出来的，所以其实卷积和互相关没啥区别</p><h2 id="将卷积运算转为矩阵相乘"><a href="#将卷积运算转为矩阵相乘" class="headerlink" title="将卷积运算转为矩阵相乘"></a>将卷积运算转为矩阵相乘</h2><p>这里先扯一扯之前没怎么听过但突然看到的<strong>量化</strong>概念。</p><p>传统优化矩阵乘的思想有基于算法分析的，也有基于软件优化的方法如改进访存局部性、利用向量指令等，这两个方法都是基于对计算机运行特性进行的改进。</p><p>而随着深度学习技术的演进，神经网络技术出现了一个重要的方向——<strong>神经网络量化</strong>，。量化技术的出现使得我们可以在深度学习领域使用一些特别的方法来优化矩阵乘，例如facebook开源的专门用于量化神经网络的计算加速库<code>QNNPACK</code>。</p><p>神经网络计算一般都是以单精度浮点(Floating-point 32, <code>FP32</code>)为基础。而网络算法的发展使得神经网络对计算和内存的要求越来越大，以至于移动设备根本无力承受。为了提升计算速度，<strong>量化</strong>(Quantization)被引入到神经网络中，主流的方法是将神经网络算法中的权重参数和计算都从<code>FP32</code>转换到<code>INT8</code>。这里有个相关的论文：<a href="http://bbs.cvmart.net/topics/1980">CVPR2020 | 8 比特数值也能训练模型？商汤提出训练加速新算法 </a>（反正我是看不明白，就是提提相关概念）</p><hr><p>下面讲解传统的卷积运算怎么转化为矩阵乘：</p><p><strong>传统的卷积核依次滑动的计算方法很难加速</strong>。转化为矩阵乘法之后，就可以调用各种线性代数运算库，<code>CUDA</code>里面的矩阵乘法实现。这些矩阵乘法都是极限优化过的，比暴力计算快很多倍。下面的两个图充分说明了转化为矩阵乘法后的具体过程：<br><img src="https://i.loli.net/2020/05/18/dNVHwc35fZxDM68.jpg" alt="在这里插入图片描述"></p><p><img src="https://i.loli.net/2020/05/18/yLEnoH2rAtQSNcX.jpg" alt="在这里插入图片描述"></p><p>细细体会！</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/31727402">(二)计算机视觉四大基本任务(分类、定位、检测、分割)</a><br><a href="https://blog.csdn.net/qq_19094871/article/details/103117936">卷积与互相关计算</a><br><a href="https://jackwish.net/2019/gemm-optimization.html">通用矩阵乘（GEMM）优化算法</a><br><a href="https://zhuanlan.zhihu.com/p/30086163">【算法】卷积(convolution)/滤波（filter）和互相关(cross-correlation)以及实现</a><br><a href="https://www.zhihu.com/question/22298352">如何通俗易懂地解释卷积？</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>13-感受野如何计算?</title>
    <link href="/2022/11/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/13_%E5%BC%84%E6%98%8E%E7%99%BD%E6%84%9F%E5%8F%97%E9%87%8E%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%A1%E7%AE%97%E9%97%AE%E9%A2%98/"/>
    <url>/2022/11/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/13_%E5%BC%84%E6%98%8E%E7%99%BD%E6%84%9F%E5%8F%97%E9%87%8E%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%A1%E7%AE%97%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="感受野Receptive-field-RF-的概念"><a href="#感受野Receptive-field-RF-的概念" class="headerlink" title="感受野Receptive field (RF)的概念"></a>感受野Receptive field (RF)的概念</h2><p>卷及神经网络中每一层输出的特征图(feature map)中的每一个像素映射到原始输入图像的区域大小。</p><h2 id="卷积输入输出的大小关系"><a href="#卷积输入输出的大小关系" class="headerlink" title="卷积输入输出的大小关系"></a>卷积输入输出的大小关系</h2><p>根据感受野的概念，大家可以体会到感受野的计算应该与卷积的计算是相反的过程，所以先回顾下卷积输入输出的大小关系公式：（以高度为例）<br>$$<br>Height_{out} = (Height_{in} - F+2*P)/S + 1<br>$$<br>其中<code>F</code>为滤波器的边长，<code>P</code>为padding的大小，<code>S</code>为步长。</p><h2 id="感受野的计算"><a href="#感受野的计算" class="headerlink" title="感受野的计算"></a>感受野的计算</h2><p>上面说了感受野的计算是与卷积计算相反的过程，卷积是从低层慢慢一层层卷积到高层的，所以感受野的计算采用的是<code>Top-Down</code>方式，即从最高层向低层迭代计算。具体步骤为：</p><ol><li><p>设要计算感受野的这层为第N层</p></li><li><p>第<code>N</code>层到第<code>N-1</code>层的感受野就是对第<code>N-1</code>层进行卷积时使用的滤波器大小，这里我们设为$RF_{N-1}$。</p></li><li><p>接着计算第<code>N</code>层到第<code>N-2</code>层的感受野大小，公式是：<br>$$RF_{N-2} = (RF_{N-1} -1)*stride + kernel_size$$ <strong>（需要注意的是这里的<code>stride</code>和<code>kernel_size</code>是第<code>N-2</code>层的）</strong></p></li></ol><p> <strong>相当于一个窗口长为2的滑窗，由后一个推出前一个。</strong></p><ol start="4"><li>一直迭代第３步直至输入层，即可算出第N层的感受野</li></ol><p>这里大家注意下第３步中的公式，体会下是不是刚好与上面卷积输入输出的关系刚好反过来，$RF_{N-2}$对应$Height_{in}$，$RF_{N-1}$对应$Height_{out}$。</p><p>唯一的区别是不需要管padding，这也说明了感受野其实是包括padding在内的，所以你会发现算出来的感受野大小可能会比原始图像的大小还要大。</p><p>如上一段所说，<strong>其实这样的计算方法是有一点问题的，没有考虑padding和pooling部分，要做修改。但这种方法还是可以应对面试时候的感受野计算问题的</strong>，若是研究过程中要计算非常准确的感受野大小的话，还是得再深入研究下，大家可以看看下面的两个参考资料。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/31004121">如何计算感受野(Receptive Field)——原理</a><br><a href="http://zike.io/posts/calculate-receptive-field-for-vgg-16/">Calculate Receptive Field for VGG16</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>12-为什么CNN在CV领域表现优秀？</title>
    <link href="/2022/11/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/12_CNN%E5%9C%A8%E5%9B%BE%E5%83%8F%E4%B8%8A%E8%A1%A8%E7%8E%B0%E5%A5%BD%E7%9A%84%E5%8E%9F%E5%9B%A0/"/>
    <url>/2022/11/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/12_CNN%E5%9C%A8%E5%9B%BE%E5%83%8F%E4%B8%8A%E8%A1%A8%E7%8E%B0%E5%A5%BD%E7%9A%84%E5%8E%9F%E5%9B%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>一提到计算机视觉，第一反应就是 CNN ，但是大家有没有想过，为什么图像识别领域的网络结构都是使用的 CNN 呢，或者说 CNN 网络有哪些特点可以使其在图像识别领域表现良好？这个问题对于我们而言都习惯到理所当然了，所以面试官要是突然问这种问题，估计很多同学都得懵逼一会。</p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>网上没有找着相关的问题答案，所以下面的分析仅仅是我个人的观点，大家交流交流，有其他见解的话非常欢迎提出来！</p><p>为什么图像识别领域要使用 CNN ，其实潜在意思是在问 CNN 中的卷积层与全连接层相比好在哪里？为什么这么说，因为在卷积神经网络之前，一般的网络都采用的是全连接的方式，前一层的每一个单元都对下一层的每一个单元有影响，而 CNN 中虽然存在全连接层，但是更核心的是前面用于提取特征的卷积层，<strong>所以这个问题便转换成了卷积层和全连接层的比较问题。</strong></p><h2 id="卷积层和全连接层的区别"><a href="#卷积层和全连接层的区别" class="headerlink" title="卷积层和全连接层的区别"></a>卷积层和全连接层的区别</h2><p>卷积层相比于全连接层，主要有两个特点：</p><ol><li><strong>局部连接：</strong>全连接层是一种稠密连接方式，而卷积层却只使用卷积核对局部进行处理，这种处理方式其实也刚好对应了图像的特点。在视觉识别中，<strong>关键性的图像特征、边缘、角点等只占据了整张图像的一小部分</strong>，相隔很远的像素之间存在联系和影响的可能性是很低的，而局部像素具有很强的相关性。</li><li><strong>共享参数：</strong>如果借鉴全连接层的话，对于1000×1000大小的彩色图像，一层全连接层便对应于三百万数量级维的特征，即会导致庞大的参数量，不仅计算繁重，还会导致过拟合。而卷积层中，卷积核会与局部图像相互作用，是一种稀疏连接，大大减少了网络的参数量。另外从直观上理解，依靠卷积核的滑动去提取图像中不同位置的相同模式也刚好符合图像的特点，不同的卷积核提取不同的特征，组合起来后便可以提取到高级特征用于最后的识别检测了。</li></ol><p>所以 CNN 应用的条件一般是要求卷积对象有局部相关性，这正是图像所具备的，因此图像领域都是使用 CNN 就解释地通了。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.zhihu.com/question/38544669">CNN为什么能使用在NLP？</a><br><a href="https://zhuanlan.zhihu.com/p/31727402">(二)计算机视觉四大基本任务(分类、定位、检测、分割)</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>11-FLops及Parameters计算</title>
    <link href="/2022/11/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/11_CNN%E7%BD%91%E7%BB%9C%E5%90%84%E7%A7%8D%E5%B1%82%E7%9A%84FLOPs%E5%92%8C%E5%8F%82%E6%95%B0%E9%87%8Fparas%E8%AE%A1%E7%AE%97/"/>
    <url>/2022/11/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/11_CNN%E7%BD%91%E7%BB%9C%E5%90%84%E7%A7%8D%E5%B1%82%E7%9A%84FLOPs%E5%92%8C%E5%8F%82%E6%95%B0%E9%87%8Fparas%E8%AE%A1%E7%AE%97/</url>
    
    <content type="html"><![CDATA[<h2 id="FLOPs"><a href="#FLOPs" class="headerlink" title="FLOPs"></a>FLOPs</h2><blockquote><p><strong>这里先注意一下FLOPs的写法，不要弄混了：</strong><br>**FLOPS(全大写)**：是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度，是一个衡量硬件性能的指标。<br>**FLOPs(s小写)**：，是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量，可以用来衡量算法/模型的复杂度，也就是我们这里要讨论的。</p></blockquote><h3 id="标准卷积层的FLOPs"><a href="#标准卷积层的FLOPs" class="headerlink" title="标准卷积层的FLOPs"></a>标准卷积层的FLOPs</h3><p>卷积层 wx + b 需要计算两部分，首先考虑前半部分 wx 的计算量(+b为偏置计算量），可得通式</p><p><strong>通式：</strong>Flops =  计算每个输出特征值的 乘法次数 + 加法次数 + 通道合并次数(忽略) + 偏置(输出通道数)</p><p>对于一个输出为WxHxC的Featture map，我们可以先计算得到一个通道上一个点(pixel)所用的计算量,再计算特征图所有点的<br>$$<br>乘法运算数M=k_{w}<em>k_h</em>C_{in} \<br>加法运算数A=（k_{w}<em>k_h-1）</em>C_{in} \<br>通道运算数C=C_{in} - 1 \<br>则 Flops= (M+A+C+1(bisa))<em>W_{out}<em>H_{out}<em>C_{out}\<br>化简得到：Flops = (2</em>k_w</em>k_h</em>C_{in})*W_{out}*H_{out}*C_{out}<br>$$</p><p>考虑bias：$(2*C_{int}*k^2)<em>C_{out}<em>H</em>W$<br>不考虑bias：$(2</em>C_{int}*k^2-1)*C_{out}<em>H</em>W$</p><p>参数定义（下同）：$C_{int}$为输入通道数，k为卷积核边长， $C_{out}$为输出通道数，H*W为输出特征图的长宽。</p><p>其实卷积层在实现的时候可以选择加bias或者不加，在很多的框架当中是一个可以选择的参数，为了严谨，这里特地提一下。</p><p>怎么理解上面的公式呢？以不考虑bias为例。我们先计算输出的feature map中的一个pixel的计算量，然后再乘以feature map的规模大小即可，所以我们主要分析下上面公式中的括号部分：<br>$$<br>(2*C_{int}*k^2-1) = C_{int}*k^2 + C_{int}*k^2-1<br>$$</p><p>可以看到我们把它分成了两部分，<strong>第一项是乘法运算数，第二项是加法运算数，因为n个数相加，要加n-1次，所以不考虑bias，会有一个-1，如果考虑bias，刚好中和掉。</strong></p><h3 id="深度可分离卷积的FLOPs"><a href="#深度可分离卷积的FLOPs" class="headerlink" title="深度可分离卷积的FLOPs"></a>深度可分离卷积的FLOPs</h3><p>深度可分离卷积分成两部分，一部分是分通道卷积，另一部分是1*1卷积。（如果不知道深度可分离卷积的朋友可以先看下<a href="https://baijiahao.baidu.com/s?id=1634399239921135758&wfr=spider&for=pc">这个博客</a>，这是一种可大大减少计算量的卷积方法）</p><p><img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/%25E5%25BE%25AE%25E4%25BF%25A1%25E6%2588%25AA%25E5%259B%25BE_20220707161709.png" alt="微信截图_20220707161709"></p><p>这里的讨论以考虑bias为准：<br>第一部分：$(2<em>k^2 )<em>H</em>W</em>C_{int}$<br>第二部分：$2<em>C_{int}<em>H</em>W</em>C_{out}$</p><p>最终的结果就是两部分相加。</p><h3 id="池化层的FLOPS"><a href="#池化层的FLOPS" class="headerlink" title="池化层的FLOPS"></a>池化层的FLOPS</h3><p>这里又分为全局池化和一般池化两种情况：</p><h4 id="全局池化"><a href="#全局池化" class="headerlink" title="全局池化"></a>全局池化</h4><p>针对输入所有值进行一次池化操作，不论是max、sum还是avg，都可以简单地看做是只需要对每个值算一次。</p><p>所以结果为：$H_{int}*W_{int}*C_{int}$</p><h4 id="一般池化"><a href="#一般池化" class="headerlink" title="一般池化"></a>一般池化</h4><p>答案是：$k^2*H_{out}*W_{out}*C_{out}$</p><p>注意池化层的：$C_{out} = C_{int}$</p><h3 id="全连接层的FLOPs"><a href="#全连接层的FLOPs" class="headerlink" title="全连接层的FLOPs"></a>全连接层的FLOPs</h3><p>考虑bias：$(2*I)<em>O$<br>不考虑bias：$(2</em>I-1)*O$</p><p>分析同理，括号内是一个输出神经元的计算量，拓展到O个输出神经元。（如果该全连接层的输入是卷积层的输出，需要先将输出展开成一列向量）</p><h3 id="激活层的FLOPs"><a href="#激活层的FLOPs" class="headerlink" title="激活层的FLOPs"></a>激活层的FLOPs</h3><h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h4><p>ReLU一般都是跟在卷积层的后面，这里假设卷积层的输出为$H<em>W</em>C$，因为ReLU函数的计算只涉及到一个判断，因此计算量就是$H<em>W</em>C$</p><h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p>根据sigmoid的公式可以知道，每个输入都需要经历4次运算，因此计算量是$H<em>W</em>C*4$（参数含义同ReLU）</p><hr><h2 id="参数量"><a href="#参数量" class="headerlink" title="参数量"></a>参数量</h2><h3 id="卷积层的参数量"><a href="#卷积层的参数量" class="headerlink" title="卷积层的参数量"></a>卷积层的参数量</h3><p>卷积层的参数量与输入特征图大小无关</p><p>考虑bias：$(k^2*C_{int}+1)<em>C_{out}$<br>不考虑bias：$(k^2</em>C_{int})*C_{out}$</p><h3 id="深度可分离卷积的参数量"><a href="#深度可分离卷积的参数量" class="headerlink" title="深度可分离卷积的参数量"></a>深度可分离卷积的参数量</h3><p>不考虑bias：<br>第一部分：$k^2<em>C_{int}$<br>第二部分：$(1</em>1*C_{int})*C_{out}$<br>最终结果为两者相加。</p><h3 id="池化层的参数量"><a href="#池化层的参数量" class="headerlink" title="池化层的参数量"></a>池化层的参数量</h3><p>池化层没有需要学习的参数，所以参数量为0。</p><h3 id="全连接层的参数量"><a href="#全连接层的参数量" class="headerlink" title="全连接层的参数量"></a>全连接层的参数量</h3><p>考虑bias：$I*O+1$</p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.zhihu.com/question/65305385/answer/451060549">CNN 模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>10-优化器原理及其发展</title>
    <link href="/2022/11/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/10_%E4%BC%98%E5%8C%96%E5%99%A8%E5%8E%9F%E7%90%86%E5%8F%8A%E5%8F%91%E5%B1%95%E8%B7%AF%E7%BA%BF/"/>
    <url>/2022/11/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/10_%E4%BC%98%E5%8C%96%E5%99%A8%E5%8E%9F%E7%90%86%E5%8F%8A%E5%8F%91%E5%B1%95%E8%B7%AF%E7%BA%BF/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>深度学习中有很多优化函数，常见的那些你还记得它的定义以及优缺点吗？</p><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><p>深度学习网络训练中，有很多可供选择的优化函数如SGD、Adam等等，到底用哪个好呢？其实这个问题没有确切的答案，优化函数是需要配合损失函数使用的，说白了，优化函数也是一种超参数，是需要尝试的，哪个效果好就用哪个……</p><p>这些优化函数其实差别不大，都是基于一个基本框架来演进的，所以下面先介绍下优化算法的基本框架：</p><h3 id="１、优化算法基本框架"><a href="#１、优化算法基本框架" class="headerlink" title="１、优化算法基本框架"></a>１、优化算法基本框架</h3><p>（记住这个框架！！！）</p><p>假设当前时刻待优化的参数为 $\theta_t$ ，损失函数为 $J(\theta)$ ，学习率为 $\eta$ ，参数更新的框架为：</p><ol><li><p>计算损失函数关于当前参数的梯度：$g_t = \nabla J(\theta_t)$</p></li><li><p>根据历史梯度计算一阶动量和二阶动量：<br>$$<br>m_t = \phi(g_1, g_2, …,g_t)\<br>V_t = \psi(g_1,g_2,…,g_t)<br>$$<br>即一阶动量为包含当前梯度在内的历史梯度的一次函数，而二阶动量是历史梯度的二次函数。</p></li><li><p>计算当前时刻的下降梯度：<br>$$<br>\Delta \theta_t = -\eta*\frac{m_t}{\sqrt{V_t}}<br>$$</p></li><li><p>根据下降梯度更新参数：$\theta_{t+1} = \theta_t + \Delta \theta_t$</p></li></ol><h3 id="２、指数加权移动平均值"><a href="#２、指数加权移动平均值" class="headerlink" title="２、指数加权移动平均值"></a>２、指数加权移动平均值</h3><p><strong>SGD只计算当前梯度更新参数，完全没有考虑历史梯度</strong>，但这样有一个问题是假如当前参数处在损失函数的局部最低点处，即梯度为0，因为梯度为0，所以参数不再更新，也就是说不管你之前历史梯度多大，下降地多快，只要你当前梯度为0，那就只能停在这里，也就意味着冲不出这个局部最低点。要解决这个问题就需要将历史梯度考虑进来，但是这里又有一个问题：历史梯度那么多，全部都考虑吗，还是只考虑一部分？其实我们只要考虑最近的一段历史梯度即可，这段历史梯度怎么截就用到了<strong>指数加权移动平均值</strong>的概念。</p><p>假设 $\upsilon_{t-1}$ 是 $t-1$ 时刻的指数加权移动平均值，$\theta_t$ 是当前 $t$ 时刻的观测值，那么 $t$ 时刻的指数加权移动平均值为：<br>$$<br>\upsilon_t = \beta \upsilon_{t-1} + ( 1- \beta)\theta_t \<br>\quad= ( 1- \beta)\theta_t + ( 1- \beta)\beta\theta_{t-1}+ \beta^2\upsilon_{t-2}\<br>\quad= ( 1- \beta)\theta_t + ( 1- \beta)\beta\theta_{t-1}+ ( 1- \beta)\beta^2\theta_{t-2}+ \beta^3\upsilon_{t-3} \<br>…(递推)\<br> = (1-\beta)\theta_t + \sum_{i=1}^{t-1}(1-\beta)\beta^i \theta_{t-i}<br>$$<br>其中 $0 \leq \beta \le 1$ ，从指数加权移动平均值的最终形式可以看出，$i$ 表示的是距离当前时刻的时间长短，$i$ 越大代表着距离当前时刻越久远，且由于其系数中指数部分的影响，其系数越小，也就是说距离当前时刻越远的历史梯度对当前时刻的指数加权移动平均值的贡献越少，这时候若我们设置一个阈值来对贡献量进行筛选，便使得当前时刻的指数加权移动平均值只考虑距离当前时刻较近的那些历史梯度，这就对应了名字中的“移动”这个概念。</p><p>除了第 $t$ 时刻的观测值权重为 $1-\beta$ 外，其他时刻的观测值权重为 $(1-\beta)\beta^i$ 。由于通常对于那些权重小于 $\frac{1}{e}$ 的观测值可以忽略不计，所以忽略掉那些权重小于这个阈值的观测值之后，上式就可以看做是在求指数加权<strong>移动</strong>平均值。</p><p> 下面我们计算一下什么时候权重 $(1-\beta)\beta^i$ 等于$\frac{1}{e}$ 的。</p><p>高数中有一个重要极限公式：<br>$$<br>\qquad \qquad\qquad\qquad\qquad\qquad\lim_{n \rightarrow \infty}(1+\frac{1}{n})^n= e \<br>其实这个极限无论是对于+\infty还是-\infty都是成立的，因此我们令 t=-n，得:\<br>lim_{t \rightarrow -\infty} (1-\frac{1}{t})^{-t} = e\quad \rightarrow \quad lim_{n \rightarrow \infty} (1-\frac{1}{n})^{n} = \frac{1}{e} \approx 0.3679 \<br>令 n=\frac{1}{1-\beta}，则: \<br>lim_{n \rightarrow \infty} (1-\frac{1}{n})^{n} = lim_{\beta \rightarrow 1}(\beta)^{\frac{1}{1-\beta}} = \frac{1}{e}<br>$$ {l}<br>所以当 $\beta \rightarrow1$ 时，那些 $i \ge \frac{1}{1-\beta}$ 的 $\theta_{t-i}$ 的权重 $(1-\beta)\beta^i$ 的权重肯定小于 $\frac{1}{e}$ 。**$\beta$ 通常取0.9，也就是说 $i \ge 10$ 的那些观测值都会被忽略，也就相当于只考虑包括当前时刻在内的最近10个时刻的指数加权移动平均值。**</p><p>但是还有一个问题是：当 t 比较小时，指数加权移动平均值的偏差较大，例如：设 $\theta_1=40,\beta=0.9$ ，那么 $v_1 = \beta v_0 + (1-\beta)\theta_1 = 0.9<em>0+0.1</em>40 = 4$ ，显然 $v_1$ 和 $\theta_1$ 相差太大，所以通常会加上一个修正因子 $1-\beta^t$ ，加上修正因子后的公式为：<br>$$<br>\upsilon_t = \frac{\beta \upsilon_{t-1} + ( 1- \beta)\theta_t}{1-\beta^t} \<br>$$<br>显然，当 t 较小时，修正因子  $1-\beta^t$ 会起作用，当 t 足够大后，$\beta^t \rightarrow 0, (1-\beta^t) \rightarrow 1$ ，修正因子自动退场。<strong>加修正因子的这个做法只有在 Adam 和 Nadam 中使用到，其他算法并没有考虑。</strong></p><h2 id="SGD-Stochastic-Gradient-Descent"><a href="#SGD-Stochastic-Gradient-Descent" class="headerlink" title="SGD (Stochastic Gradient Descent)"></a>SGD (Stochastic Gradient Descent)</h2><p>SGD不考虑历史梯度，所以当前时刻的一阶动量即为当前时刻的梯度 $m_t = g_t$ ，且二阶动量 $V_t = E$ ，所以SGD的参数更新公式为：<br>$$<br>\Delta \theta_t = -\eta<em>\frac{g_t}{\sqrt{E}} = -\eta</em>g_t \<br>\theta_{t+1}= \theta_t + \Delta\theta_t = \theta_t-\eta*g_t<br>$$<br><strong>缺点</strong>：容易陷入局部最优。由于SGD只考虑当前时刻的梯度，在局部最优点的当前时刻梯度为 0 ，根据上面计算公式可知，此时参数不再进行更新，故陷入局部最优的状态。</p><p>但是虽然SGD有陷入局部最优的缺陷，但还是很常用。我的理解是：以上分析是针对一个参数 $\theta_i$ 来说的，即使其中一个参数陷入局部最优，但其他参数还是会继续更新，所以大概率会将陷入局部最优的那个参数拖离局部最优点，于是该参数可以继续更新。 所以整体来说并不会像单个参数那样陷入局部最优就出不来，所以还是可以work的。</p><h3 id="改进策略及算法"><a href="#改进策略及算法" class="headerlink" title="改进策略及算法"></a>改进策略及算法</h3><ol><li><strong>引入历史梯度的一阶动量</strong>，代表算法：Momentum、NAG</li><li><strong>引入历史梯度的二阶动量</strong>，代表算法：AdaGrad、RMSProp、AdaDelta</li><li><strong>同时引入历史梯度的一阶动量及二阶动量</strong>，代表算法：Adam、Nadam</li></ol><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>为了抑制SGD的震荡（有点不理解这句话），Momentum认为梯度下降过程可以加入惯性，也就是在SGD的基础上引入了一阶动量。而所谓的一阶动量就是该时刻梯度的指数加权移动平均值，而由于此时仍然没有用到二阶动量，所以 $V_t=E$ ，所以Momentum的参数更新公式如下：<br>$$<br>m_t = \beta m_{t-1}+\eta g_t \<br>\Delta\theta_t = -\eta<em>\frac{m_t}{\sqrt{E}} = -\eta</em>m_t = -(\beta m_{t-1}+\eta g_t) \(这里m_t乘以\eta后可以视为不变，因为乘上后，系数同样是大于0小于1的) \<br>\theta_{t+1} = \theta_t -(\beta m_{t-1}+\eta g_t)<br>$$<br>可以看到上面式子的**第一行 $g_t$ 前面的系数并不是严格按照我们上面指数加权移动平均值的定义采用权重 $1-\beta$ ，而是使用我们自定义的学习率 $\eta$ **，这点需要注意。 </p><h2 id="NAG-Nesterov-Accelerated-Gradient"><a href="#NAG-Nesterov-Accelerated-Gradient" class="headerlink" title="NAG (Nesterov Accelerated Gradient)"></a>NAG (Nesterov Accelerated Gradient)</h2><p>除了利用惯性（一阶动量）跳出局部沟壑外，我们还可以尝试往前看一步，即：在Momentum考虑历史梯度的基础上，把当前梯度转换为未来梯度。</p><p>想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前多看一步。我们知道Momentum在时刻 $t$ 的主要下降方向是由历史梯度（惯性）决定的，当前时刻的梯度权重较小，那不如不用管当前梯度，而是先看看如果跟着惯性走了一步，那个时候外面的世界是怎样的。也即在Momentum的基础上将当前时刻的梯度换成下一时刻的梯度。由于此时仍然没有用到二阶动量，所以 $V_t=E$ ，所以NAG的参数更新公式为：<br>$$<br>Momentum中原本下一个时刻的梯度计算公式为:\theta_{t+1} = \theta_t -(\beta m_{t-1}+\eta g_t) \<br>不考虑当前梯度即令;g_t = 0 \<br>所以下一个时刻的梯度的计算公式为：　\theta_{t+1} = \theta_t -\beta m_{t-1} \<br>所以将当前时刻的梯度换成下一时刻的梯度即:g_t = \Delta J(\theta_t -\beta m_{t-1})\<br>上式代入到Momentum的参数更新公式中：\theta_{t+1} = \theta_t -(\beta m_{t-1}+\eta \Delta J(\theta_t -\beta m_{t-1}))<br>$$</p><hr><blockquote><p>以上的两个概念只引入了一阶动量。而二阶动量的出现，才意味着<strong>“自适应学习率”</strong>优化算法时代的到来。在SGD及其引入一阶动量的改进算法中，均以相同的学习率去更新参数。但是，以相同的学习率进行变化经常是不合理的。</p><p>在神经网络中，参数需要用不同的学习率进行更新。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于那些偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。</p><p>以神经网络中的 W 及 b 为例，如下图为损失函数等高线图，W 为横轴，b 为纵轴。发现每次b 变化很大，而 W 每次仅更新一小步。但是，纵观整个损失函数我们发现，W 其实可以迈开步子往前走，而 b 则不用那么活跃。</p><p><img src="https://i.loli.net/2020/05/14/KXrhZpb4CSTYJRz.jpg" alt="img"></p><p>于是，出现了以下针对不同维度的参数采用不同学习率的二阶动量改进算法。</p></blockquote><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>从数学的角度来看，更新幅度很大的参数，通常其历史累计梯度的平方和会很大；相反，更新幅度很小的参数，通常其累计历史梯度的平方和会很小。因此，我们可以考虑让学习率除以历史梯度的平方和，这样之前更新幅度大的参数的学习率分母也大，之前更新幅度小的参数的学习率分母也小，从而起到调节学习率的效果。</p><p>我们上面讨论的 $\theta_t$ 指的是网络中的参数，但是参数有很多个，所以其实 $\theta_t$ 是一个向量，我们假设网络中有 d 个参数，那么 $\theta_t = [\theta_{t,1}, \theta_{t,2},..\theta_{t,d}]^T$ 。那么针对其中的第 i 维度的参数梯度更新公式为：<br>$$<br>\upsilon_{t,i} = \sum_{t=1}^{t}g_{t,i}^2 \<br>\Delta \theta_{t,i} = -\frac{\eta}{\sqrt{\upsilon_{t,i}+\varepsilon}}*g_{t,i} \ \theta_{t+1, i}=\theta_{t,i}-\frac{\eta}{\sqrt{\upsilon_{t,i}+\varepsilon}}*g_{t,i}<br>$$<br>其中 $g_{t,i}$ 表示第 t 时刻第 i 维度参数的梯度值，$\varepsilon$ 是防止分母等于 0 的平滑项（常取一个很小的值例如 1e-8） 。显然，此时上式中的 $-\frac{\eta}{\sqrt{\upsilon_{t,i}+\varepsilon}}$ 这个整体可以看作是学习率，分母中的历史累计梯度值 $v_{t,i}$ 越大的参数学习率越小。</p><p>上式仅仅是第 t 时刻第 i 维度参数的更新公式，对于第 t 时刻所有维度的参数的更新公式如下：<br>$$<br>V_t = diag(\upsilon_{t,1}, \upsilon_{t,2},…,v_{t,d}) \in R^{d*d} \<br>\Delta \theta_{t} = -\frac{\eta}{\sqrt{V_t+\varepsilon}}*g_{t} \<br>\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{V_t+\varepsilon}}*g_{t}<br>$$<br>也就是构造成矩阵相乘的形式：　$V_t$ 是对角矩阵（除了对角线有非零值外其他地方都是 0 ） 所以上式中的 $\varepsilon$ 只用来平滑 $V_t$ 对角线上的元素。</p><p><strong>缺点</strong>：随着时间步的拉长，历史累计梯度平方和 $v_{t,i}$ 会越来越大，这样会使得所有维度参数的学习率都不断减小（单调递减），无论更新幅度如何。</p><h2 id="RMSProp-AdaDelta"><a href="#RMSProp-AdaDelta" class="headerlink" title="RMSProp / AdaDelta"></a>RMSProp / AdaDelta</h2><p>由于 AdaGrad 单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累计全部历史梯度，而只关注过去一段时间窗口的下降梯度，采用 Momentum 中的指数加权移动平均值的思路。</p><p>首先看最简单直接版的 <strong>RMAProp</strong> ，RMSProp 就是在 AdaGrad 的基础上将普通的历史累计梯度平方和换成历史累计梯度平方和的指数加权移动平均值，所以只需将 AdaGrad 中的 $v_{t,i}$ 的公式改成指数加权移动平均值的形式即可：<br>$$<br>v_{t,i} = \beta v_{t-1,i} + (1-\beta)g_{t,i}^2 \<br>V_t = diag(\upsilon_{t,1}, \upsilon_{t,2},…,v_{t,d}) \in R^{d*d} \<br>\Delta \theta_{t} = -\frac{\eta}{\sqrt{V_t+\varepsilon}}*g_{t} \<br>\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{V_t+\varepsilon}}*g_{t}<br>$$<br>而 <strong>AdaDelta</strong> 又在 RMSProp 的基础上进行改进：它除了对二阶动量计算指数加权移动平均值之外，还对学习率动了手脚，即要达到的目标是不需要我们人为设定固定的学习率，而是让模型根据历史经验将学习率给换掉。所以另外它会对当前时刻的下降梯度 $\Delta \theta_t$ 的平方也计算一个指数加权移动平均，具体的：<br>$$<br>E[\Delta \theta^2]<em>{t,i} = \gamma E[\Delta \theta^2]</em>{t-1,i} + (1-\gamma)\Delta \theta_{t,i}^2<br>$$<br>由于 $\Delta \theta_{t,i}^2$ 目前是未知的，所以只能用 t-1 时刻的指数加权移动平均值来近似替换，也即：<br>$$<br>E[\Delta \theta^2]<em>{t-1,i} = \gamma E[\Delta \theta^2]</em>{t-2,i} + (1-\gamma)\Delta \theta_{t-1,i}^2<br>$$<br>接着 AdaDelta 将此值替换我们预先设置的学习率 $\eta$ 。</p><p>因此，AdaDelta 的参数更新公式如下：<br>$$<br>v_{t,i} = \beta v_{t-1,i} + (1-\beta)g_{t,i}^2 \<br>V_t = diag(\upsilon_{t,1}, \upsilon_{t,2},…,v_{t,d}) \in R^{d<em>d} \<br>E[\Delta \theta^2]<em>{t-1,i} = \gamma E[\Delta \theta^2]</em>{t-2,i} + (1-\gamma)\Delta \theta_{t-1,i}^2 \<br>\Theta_t = diag(E[\Delta \theta^2]<em>{t-1,1}, E[\Delta \theta^2]</em>{t-1,2}, …,E[\Delta \theta^2]_{t-1,d}) \in R^{d</em>d}\<br>\Delta \theta_{t} = -\frac{\sqrt{\Theta_t + \varepsilon}}{\sqrt{V_t+\varepsilon}}*g_{t} \<br>\theta_{t+1}=\theta_{t}-\frac{\sqrt{\Theta_t + \varepsilon}}{\sqrt{V_t+\varepsilon}}*g_{t}<br>$$<br>显然，对于 AdaDelta 算法来说，已经不需要我们自己预设学习率了，只需要预设 $\beta$ 和 $\gamma$ 这两个指数加权移动平均值的衰减率即可。</p><hr><blockquote><p>下面的两个算法对SGD的改进策略是同时引入一阶动量和二阶动量。Adam和Nadam都是前述方法的集大成者。</p></blockquote><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>在 RMSProp 的基础上再考虑一阶动量(Momentum)。具体如下：</p><p>首先计算一阶动量：（注意这个公式中 $g_t$ 前面的系数与Momentum是不同的）<br>$$<br>m_t = \beta_1 m_{t-1}+(1-\beta_1)g_t<br>$$<br>然后类似 RMSProp 和 AdaDelta 计算二阶动量：<br>$$<br>v_{t,i} = \beta_2 v_{t-1,i} + (1-\beta_2)g_{t,i}^2 \<br>V_t = diag(\upsilon_{t,1}, \upsilon_{t,2},…,v_{t,d}) \in R^{d*d} \<br>$$<br><strong>但是这里要加上修正因子</strong>，即：（如果忘了这个回到前面去再看看指数加权移动平均值概念的最后部分）<br>$$<br>\hat{m}<em>t = \frac{m_t}{1-\beta_1^t} \<br>\hat{v}</em>{t,i} = \frac{v_{t,i}}{1-\beta_2^t} \<br>\hat{V}<em>t = diag(\hat{v}</em>{t,1}, \hat{v}<em>{t,2},…,\hat{v}</em>{t,d}) \in R^{d*d} \<br>$$<br>所以，Adam的参数更新公式为：<br>$$<br>\Delta \theta_{t} = -\frac{\eta}{\sqrt{\hat{V}<em>t+\varepsilon}}*\hat{m}</em>{t} \<br>\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{V}<em>t+\varepsilon}}*\hat{m}</em>{t}<br>$$</p><h2 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h2><p>从这名字也能看出，Nadam = Nestrov + Adam ，具体思想如下：由于 Nesterov 的核心在于，计算当前时刻的梯度 $g_t$ 时使用了 “未来梯度” $\Delta J(\theta_t - \beta m_{t-1})$ ，Nadam 基于此提出了一种公式变形的思路，大意可以这样理解：只要能在梯度计算中考虑到 “未来梯度” ，就算达到了 Nestrov 的效果。既然如此，我们不一定非要在计算 $g_t$ 时使用 “未来梯度” ，可以考虑在其他地方使用未来梯度。</p><p>具体的，首先在 Adam 的基础上将 $\hat{m_t}$ 展开：<br>$$<br>\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{V}<em>t+\varepsilon}}*\hat{m}</em>{t}\<br>=\theta_{t}-\frac{\eta}{\sqrt{\hat{V}<em>t+\varepsilon}}*(\frac{\beta m</em>{t-1}}{1-\beta_1^t}+\frac{(1-\beta_1)g_t}{1-\beta_1^t})<br>$$<br>此时，如果我们将第 t-1 时刻的动量 $m_{t-1}$ 用第 t 时刻的动量 $m_t$ 近似代替的话，那么我们就引入了 “未来因素” ，所以便可以得到 Nadam 的表达式为：<br>$$<br>\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{V}<em>t+\varepsilon}}*(\frac{\beta m</em>{t}}{1-\beta_1^t}+\frac{(1-\beta_1)g_t}{1-\beta_1^t})<br>$$</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247493857&idx=3&sn=34e8a373adee73f6bbe6dff740b7ede5&chksm=ec1c0518db6b8c0ef85381973446ab9ab215ccc07e4c7787a7b4e5dcb873ba5d81eec3ba164c&scene=0&xtrack=1#rd">１、深度学习中的优化算法串讲</a><br><a href="https://www.bilibili.com/video/av94067702/">２、以上资料的视频讲解</a></p><p>​                                                                                                                                                                                By Yee<br>​                                                                                                                                                                            2020.05.14</p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>09-为什么输入网络要对图像作做归一化？</title>
    <link href="/2022/11/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/09_%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BE%93%E5%85%A5%E7%BD%91%E7%BB%9C%E5%89%8D%E8%A6%81%E5%AF%B9%E5%9B%BE%E5%83%8F%E5%81%9A%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <url>/2022/11/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/09_%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BE%93%E5%85%A5%E7%BD%91%E7%BB%9C%E5%89%8D%E8%A6%81%E5%AF%B9%E5%9B%BE%E5%83%8F%E5%81%9A%E5%BD%92%E4%B8%80%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在将图像输入到深度学习网络之前，一般先对图像进行预处理，即图像归一化，为什么需要这么做呢？</p><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>在面试的时候，面试官先问的问题是“<u>机器学习中为什么要做特征归一化</u>”，我的回答是“特征归一化可以消除特征之间量纲不同的影响，不然分析出来的结果显然会倾向于数值差别比较大的特征，另外从梯度下降的角度理解，数据归一化后，最优解的寻优过程明显会变得平缓，更容易正确的收敛到最优解”。接着面试官又问“<u>图像的像素值都是在0到255之间，并不存在量纲的差别，那为什么还需要做归一化呢</u>？”是啊，为什么还要呢，被问住了……</p><h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><p>既然是从机器学习特征归一化引出的图像的归一化问题，那么我们先讨论下“<u>为什么要对数值型特征做归一化</u>？”吧。</p><p>很多资料例如《百面机器学习》都是从梯度下降的角度来分析这个问题的，讨论地还不错的一篇是这个<a href="https://zhuanlan.zhihu.com/p/27627299">知乎的回答</a>，已经写得比较清晰了，所以这里就不再整理了，直接点开链接看。</p><p>不过这个回答里面未归一化时的损失函数等高线图中椭圆的方向应该是横向的而不是纵向的，因为 $\theta_2$ 前的系数比 $\theta_1$ 的大，所以在损失函数等高图上 $\theta_2$ 的变化范围比 $\theta_1$ 小才对，另外对于为什么圆形的等高线相对于椭圆形的等高线，更新方向更加平滑，所以更快也更容易收敛到最优解呢？<strong>一句话简单解释就是因为归一化后，等高图大致为圆形，更新方向与等高线垂直，所以理想的更新方向是直指圆心的一条直线。</strong></p><p>所以对于“为什么机器学习中要进行特征归一化”这个问题，总结起来可以从三个点去回答：</p><ol><li><strong>消除特征之间量纲的影响</strong>，使得不同特征之间具有可比性</li><li>在使用随机梯度下降求解的模型中，<strong>能加快模型收敛速度</strong></li><li><strong>归一化还有可能提高精度</strong>：一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。</li></ol><h2 id="问题解答"><a href="#问题解答" class="headerlink" title="问题解答"></a>问题解答</h2><p>对于这个问题，网上看了很多博客，没有找到一个很全面权威的解释，所以这里把几个自认为讲得比较合理的解释列出来：</p><ol><li><p>灰度数据表示有两种方法：一种是uint8类型、另一种是double类型。其中uint8类型数据的取值范围为 [0,255]，而double类型数据的取值范围为[0,1]，两者正好相差255倍。对于double类型数据，其取值大于1时，就会表示为白色，不能显示图像的信息，故当运算数据类型为double时，为了显示图像要除255。</p></li><li><p>图像深度学习网络也是使用gradient descent来训练模型的，使用gradient descent的都要在数据预处理步骤进行数据归一化，主要原因是，根据反向传播公式：<br>$$<br>\frac{\partial J}{\omega_{11}} = x_1*后面层梯度的乘积<br>$$<br>如果输入层 x  很大，在反向传播时候传递到输入层的梯度就会变得很大。梯度大，学习率就得非常小，否则会越过最优。在这种情况下，学习率的选择需要参考输入层数值大小，而<strong>直接将数据归一化操作，能很方便的选择学习率</strong>。在未归一化时，输入的分布差异大，所以各个参数的梯度数量级不相同，因此，它们需要的学习率数量级也就不相同。对 w1 适合的学习率，可能相对于 w2  来说会太小，如果仍使用适合 w1 的学习率，会导致在 w2 方向上走的非常慢，会消耗非常多的时间，而使用适合 w2 的学习率，对 w1  来说又太大，搜索不到适合 w1 的解。</p></li><li><p>通过标准化后，实现了数据中心化，数据中心化符合数据分布规律，能<strong>增加模型的泛化能力</strong></p></li></ol><h2 id="问题深入"><a href="#问题深入" class="headerlink" title="问题深入"></a>问题深入</h2><p>那么深度学习中在训练网络之前应该怎么做图像归一化呢？有两种方法：</p><ol><li><strong>归一化到 0 - 1</strong>：因为图片像素值的范围都在0~255，图片数据的归一化可以简单地除以255. 。 (注意255要加 . ，因为是要归一化到double型的 0-1 )</li><li>**归一化到 [-1, 1]**：在深度学习网络的代码中，将图像喂给网络前，会先统计训练集中图像RGB这3个通道的均值和方差，如：<code>mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375]</code>，接着对各通道的像素做减去均值并除以标准差的操作。不仅在训练的时候要做这个预处理，在测试的时候，同样是使用在训练集中算出来的均值与标准差进行的归一化。</li></ol><p>注意两者的区别：归一化到 [-1, 1] 就不会出现输入都为正数的情况，如果输入都为正数，会出现什么情况呢？：根据求导的链式法则，w的局部梯度是X，当X全为正时，由反向传播传下来的梯度乘以X后不会改变方向，要么为正数要么为负数，<strong>也就是说w权重的更新在一次更新迭代计算中要么同时减小，要么同时增大。</strong></p><p><img src="https://i.loli.net/2020/05/10/NFAOnSPDhZqyfIY.png" alt="img"></p><p>其中，w的更新方向向量只能在第一和第三象限。假设最佳的w向量如蓝色线所示，由于输入全为正，现在迭代更新只能沿着红色路径做zig-zag运动，更新的效率很慢。</p><p>基于此，当输入数据减去均值后，就会有负有正，会消除这种影响。</p><h3 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h3><p><a href="https://zhuanlan.zhihu.com/p/27627299">1.为什么要对数据进行归一化处理？ </a><br><a href="https://www.zhihu.com/question/293640354">2. 深度学习中图像为什么要归一化？</a><br><a href="https://blog.csdn.net/qq_32172681/article/details/100876348">3. 深度学习中，为什么需要对数据进行归一化</a><br><a href="https://blog.csdn.net/hai008007/article/details/79718251">4. 深度学习的输入数据集为什么要做均值化处理</a></p><p>​                                                                                                                                                        </p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>08-softmax函数求导过程</title>
    <link href="/2022/11/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/08_softmax%E5%87%BD%E6%95%B0%E5%8F%8A%E6%B1%82%E5%AF%BC%E8%BF%87%E7%A8%8B/"/>
    <url>/2022/11/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/08_softmax%E5%87%BD%E6%95%B0%E5%8F%8A%E6%B1%82%E5%AF%BC%E8%BF%87%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>今天小伙伴问会不会梯度求导，发现自己对离散变量求导并不熟悉，所以以somafmax为例子复习下，再进阶其实了解计算图了。</p><h2 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h2><p><strong>softmax用于多分类过程中</strong>，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类！</p><p>假设我们有一个数组$Z$，$Z_i$表示$Z$中的第i个元素，那么这个元素的softmax值就是:</p><p>​                                                                                           $$S_i = \frac {e^{z_i}}{\sum_{j}e^{z_j}}$$</p><p>更形象的如下图表示：</p><p><img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/v2-11758fbc2fc5bbbc60106926625b3a4f_r.jpg" alt="preview"></p><p>softmax直白来说就是将原来输出是3,1,-3 通过softmax函数一作用，就映射成为(0,1)的值，而这些值的累和为1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标！</p><h2 id="softmax求导"><a href="#softmax求导" class="headerlink" title="softmax求导"></a>softmax求导</h2><p>在神经网络中，我们经常可以看到以下公式，用于计算结点的激活值：</p><p><img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/20210314234609710.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/20210314234624842.png" alt="img"></p><p>计算示意图如下：</p><p><img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/202103142348336.png" alt="img"></p><p>从图中可以得到:</p><p>$z4 = w41<em>o1+w42</em>o2+w43*o3 + b14$</p><p>$z5 = w51<em>o1+w52</em>o2+w53*o3 + b25$</p><p>$z6 = w61<em>o1+w62</em>o2+w63*o3 + b36$</p><p>直接甩出<a href="https://so.csdn.net/so/search?q=Softmax&spm=1001.2101.3001.7020">Softmax</a>的公式：</p><p><img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/20210314234638888.png" alt="img"></p><p>表示类别数，z表示输出向量，zj表示向量z的第j个值。</p><p>对Softmax<a href="https://so.csdn.net/so/search?q=%E6%B1%82%E5%AF%BC&spm=1001.2101.3001.7020">求导</a><strong>：显然是目标是<img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/20210314234716409.png" alt="img">和<img src="https://img-blog.csdnimg.cn/20210314234716410.png" alt="img"></strong></p><p>根据求导的链式法则：</p><p><img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/image-20220904170229139.png" alt="image-20220904170229139"></p><p>所以核心问题就转换为求<img src="https://img-blog.csdnimg.cn/20210314234716414.png" alt="img"> ，在接触到这个式子的时候，考虑到一个问题，为什么这里是对<img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/20210314234716422.png" alt="img">求导而不是对<img src="https://img-blog.csdnimg.cn/20210314234821859.png" alt="img">求导，因为要考虑</p><p>$i = j 和 i != j$的情况。</p><img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/2021031423530680.png" alt="img" style="zoom: 67%;" /><img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/20210314235316498.png" alt="img" style="zoom:67%;" /><p><img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/image-20220904172123950.png" alt="image-20220904172123950"></p><p>到此我们就能求出w和b了的梯度用于参数更新了，注意输入x维度(3,1), w维度(3,3), b维度(3,1), </p><p>$w_{41}$的梯度是怎么计算的了，根据上面的图路线是：$s_4-&gt;z_4-&gt;o_1$, 此时i=j, 由<img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/image-20220904174600623.png" alt="image-20220904174600623" style="zoom:50%;" />，可得$s_4(1-s_4)*o1$, 其他以此内推。</p><h2 id="torch实现"><a href="#torch实现" class="headerlink" title="torch实现"></a>torch实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-comment"># 设置随机值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">seed_set</span>(<span class="hljs-params">seed</span>):<br>    np.random.seed(seed)<br>    torch.manual_seed(seed)<br>    torch.cuda.manual_seed(seed)<br>    <span class="hljs-comment">#torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.</span><br>    torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br>    torch.backends.cudnn.benchmark = <span class="hljs-literal">False</span><br><br>seed_set(<span class="hljs-number">42</span>)<br><br>w = torch.rand((<span class="hljs-number">4</span>, <span class="hljs-number">10</span>))<br>a = torch.rand((<span class="hljs-number">10</span>, <span class="hljs-number">10</span>), requires_grad=<span class="hljs-literal">True</span>)<br><br><br>b = a.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>, keepdim=<span class="hljs-literal">True</span>)<br>c = a/b<br>y = w@c<br><br>vals = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">globals</span>().values())<br><span class="hljs-keyword">for</span> xx <span class="hljs-keyword">in</span> vals:<br>  <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(xx, torch.Tensor):<br>    <span class="hljs-keyword">if</span> xx.requires_grad: xx.retain_grad()<br><br>torch.<span class="hljs-built_in">sum</span>(y).backward()<br><span class="hljs-built_in">print</span>(y.grad)<br><span class="hljs-built_in">print</span>(c.grad)<br><span class="hljs-built_in">print</span>(b.grad)<br><span class="hljs-built_in">print</span>(a.grad)<br><br>exit()<br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/25723112">详解softmax函数以及相关求导过程</a></p><p><a href="https://blog.csdn.net/ytusdc/article/details/80597945">softmax 函数以及相关求导过程+交叉熵</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>07-梯度消失和梯度爆炸的原因及解决方法</title>
    <link href="/2022/11/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/07_%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E7%88%86%E7%82%B8%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
    <url>/2022/11/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/07_%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E7%88%86%E7%82%B8%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>梯度消失无论是笔试还是面试都是常客了，其实对应于梯度消失，还有一个梯度爆炸的概念，这又是什么导致的呢？下面我们将根据公式推导来解释何为梯度消失与梯度爆炸。</p><h2 id="梯度消失和梯度爆炸的表现"><a href="#梯度消失和梯度爆炸的表现" class="headerlink" title="梯度消失和梯度爆炸的表现"></a>梯度消失和梯度爆炸的表现</h2><p>网络层数越多，模型训练的时候便越容易出现 梯度消失(gradient vanish) 和 梯度爆炸(gradient explod) 这种梯度不稳定的问题。假设现在有一个含有３层隐含层的神经网络：</p><p><img src="https://i.loli.net/2020/05/21/SArqy1bp56euwIV.png" alt="这里写图片描述"></p><p><strong>梯度消失发生时的表现是：</strong>靠近输出层的 hidden layer 3 的权值更新正常，但是靠近输入层的 hidden layer 1 的权值更新非常慢，导致其权值几乎不变，仍接近于初始化的权值。这就导致 hidden layer 1 相当于只是一个映射层，对所有的输入做了一个函数映射，这时的深度学习网络的学习等价于只有后几层的隐含层网络在学习。</p><p><strong>梯度爆炸发生时的表现是：</strong>当初始的权值太大，靠近输入层的 hidden layer 1 的权值变化比靠近输出层的 hidden layer 3 的权值变化更快。</p><p><strong>所以梯度消失和梯度爆炸都是出现在靠近输入层的参数中。</strong></p><h2 id="产生梯度消失与梯度爆炸的根本原因"><a href="#产生梯度消失与梯度爆炸的根本原因" class="headerlink" title="产生梯度消失与梯度爆炸的根本原因"></a>产生梯度消失与梯度爆炸的根本原因</h2><h3 id="梯度消失分析"><a href="#梯度消失分析" class="headerlink" title="梯度消失分析"></a>梯度消失分析</h3><p>下图是我画的一个非常简单的神经网络，每层都只有一个神经元，且神经元所用的激活函数 $\sigma$ 为 sigmoid 函数，$Loss$ 表示损失函数，前一层的输出与后一层的输入关系如下：<br>$$<br>y_i = \sigma(z_i) = \sigma(w_i*x_i+b_i), \quad其中x_i = y_{i-1}<br>$$<br><img src="https://i.loli.net/2020/05/21/aplXMCsvPw4rh2Q.jpg"></p><p>因此，根据反向传播的链式法则，损失函数相对于参数 $b_1$ 的梯度计算公式如下：<br>$$<br>\frac{\partial Loss}{\partial b_1} = \frac{\partial Loss}{\partial y_4}*\frac{\partial y_4}{\partial z_4}*\frac{\partial z_4}{\partial x_4}*\frac{\partial x_4}{\partial z_3}*\frac{\partial z_3}{\partial x_3}*\frac{\partial x_3}{\partial z_2}*\frac{\partial z_2}{\partial x_2}*\frac{\partial x_2}{\partial z_1}*\frac{\partial z_1}{\partial b_1} \<br>= \frac{\partial Loss}{\partial y_4}*\partial{‘}(z_4)<em>w_4</em>\partial{‘}(z_3)<em>w_3</em>\partial{‘}(z_2)<em>w_2</em>\partial{‘}(z_1)<br>$$<br>而 sigmoid 函数的导数 $\sigma{‘}(x)$ 如下图所示：</p><p><img src="https://i.loli.net/2020/05/21/eca5HdVqL9EBmuU.png" alt="这里写图片描述"></p><p>即 $\sigma{‘}(x)\le \frac{1}{4}$ ，而我们一般会使用标准方法来初始化网络权重，即使用一个均值为 0 标准差为 1 的高斯分布，因此初始化的网络参数 $w_i$ 通常都小于 1 ，从而有 $|\sigma{‘}(z_i)<em>w_i|\le \frac{1}{4}$ 。*<em>根据公式(2)的计算规律，层数越多，越是前面的层的参数的求导结果越小，于是便导致了梯度消失情况的出现。</em></em></p><h3 id="梯度爆炸分析"><a href="#梯度爆炸分析" class="headerlink" title="梯度爆炸分析"></a><strong>梯度爆炸分析</strong></h3><p>在分析梯度消失时，我们明白了导致其发生的主要原因是　$|\sigma{‘}(z_i)*w_i|\le \frac{1}{4}$ ，经链式法则反向传播后，越靠近输入层的参数的梯度越小。而导致梯度爆炸的原因是：$|\sigma{‘}(z_i)*w_i|&gt;1$，当该表达式大于 1 时，经链式法则的指数倍传播后，前面层的参数的梯度会非常大，从而出现梯度爆炸。</p><p>但是要使得$|\sigma{‘}(z_i)<em>w_i|&gt;1$，就得 $|w_i| &gt; 4$才行，按照 $|\sigma{‘}(w_i</em>x_i+b_i)<em>w_i|&gt;1$，可以计算出 $x_i$ 的数值变化范围很窄，仅在公式(3)的范围内，才会出现梯度爆炸，*<em>因此梯度爆炸问题在使用 sigmoid 激活函数时出现的情况较少，不容易发生。</em></em></p><p><img src="https://i.loli.net/2020/05/21/dIgKDuw976leTF4.png" alt="这里写图片描述"></p><h2 id="怎么解决"><a href="#怎么解决" class="headerlink" title="怎么解决"></a>怎么解决</h2><p>如上分析，<strong>造成梯度消失和梯度爆炸问题是网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应</strong>。另外一个原因是当激活函数使用 sigmoid 时，梯度消失问题更容易发生，因此可以考虑的解决方法如下：</p><ol><li>压缩模型层数</li><li>改用其他的激活函数如 ReLU</li><li>使用 BN 层</li><li>使用 ResNet 的短路连接结构(没有解决爆炸问题)</li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://blog.csdn.net/qq_17130909/article/details/80582226">激活函数及其作用以及梯度消失、爆炸、神经元节点死亡的解释</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
